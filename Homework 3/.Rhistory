folds = sample(1:k, nrow(train_A), replace = TRUE)
cv_log_loss = rep(0., num_param) # Store cross-validated R^2 for different parameter values
# Iterate over parameter values
for(i in 1:num_param){
# Iterate over folds to compute R^2 for parameter
for(j in 1:k){
# Fit model on all folds other than 'j' with parameter value param_val[i]
train_A_total <- colSums(train_A[folds!=j, ][-1])
train_B_total <- colSums(train_B[folds!=j, ][-1])
val_set <- rbind(train_A[folds == j, ], train_B[folds == j, ])
posterior_prob <- dmm_model(param_val[i], train_A_total, train_B_total, val_set[-1])
val_identity <- as.numeric(val_set$author == "AaronPressman")
print(posterior_prob)
# Compute R^2 for predicted values
cv_log_loss[i] = cv_log_loss[i] + log_loss(val_identity, posterior_prob)
}
# Average R^2 across k folds
cv_log_loss[i] = cv_log_loss[i] / k
}
# Return cross-validated R^2 values
return(cv_log_loss)
}
alphas <- c(1.)
dmm_cv_scores <- crossval_dmm(dataset1_train, alphas, 2)
alphas <- c(1.)
crossval_dmm = function(data, param_val, k) {
# Input:
#   Training data frame: 'train',
#   Vector of span parameter values: 'param_val',
#   Number of CV folds: 'k'
# Output:
#   Vector of R^2 values for the provided parameters: 'cv_rsq'
train_A <- data[data$author == "AaronPressman", ]
train_B <- data[data$author != "AaronPressman", ]
num_param = length(param_val) # Number of parameters
set.seed(109) # Set seed for random number generator
# Divide training set into k folds by sampling uniformly at random
# folds[s] has the fold index for train instance 's'
folds = sample(1:k, nrow(train_A), replace = TRUE)
cv_log_loss = rep(0., num_param) # Store cross-validated R^2 for different parameter values
# Iterate over parameter values
for(i in 1:num_param){
# Iterate over folds to compute R^2 for parameter
for(j in 1:k){
# Fit model on all folds other than 'j' with parameter value param_val[i]
train_A_total <- colSums(train_A[folds!=j, ][-1])
train_B_total <- colSums(train_B[folds!=j, ][-1])
val_set <- rbind(train_A[folds == j, ], train_B[folds == j, ])
posterior_prob <- dmm_model(param_val[i], train_A_total, train_B_total, val_set[-1])
val_identity <- as.numeric(val_set$author == "AaronPressman")
print(log_loss(val_identity, posterior_prob))
# Compute R^2 for predicted values
cv_log_loss[i] = cv_log_loss[i] + log_loss(val_identity, posterior_prob)
}
# Average R^2 across k folds
cv_log_loss[i] = cv_log_loss[i] / k
}
# Return cross-validated R^2 values
return(cv_log_loss)
}
alphas <- c(1.)
dmm_cv_scores <- crossval_dmm(dataset1_train, alphas, 2)
crossval_dmm = function(data, param_val, k) {
# Input:
#   Training data frame: 'train',
#   Vector of span parameter values: 'param_val',
#   Number of CV folds: 'k'
# Output:
#   Vector of R^2 values for the provided parameters: 'cv_rsq'
train_A <- data[data$author == "AaronPressman", ]
train_B <- data[data$author != "AaronPressman", ]
num_param = length(param_val) # Number of parameters
set.seed(109) # Set seed for random number generator
# Divide training set into k folds by sampling uniformly at random
# folds[s] has the fold index for train instance 's'
folds = sample(1:k, nrow(train_A), replace = TRUE)
cv_log_loss = rep(0., num_param) # Store cross-validated R^2 for different parameter values
# Iterate over parameter values
for(i in 1:num_param){
# Iterate over folds to compute R^2 for parameter
for(j in 1:k){
# Fit model on all folds other than 'j' with parameter value param_val[i]
train_A_total <- colSums(train_A[folds!=j, ][-1])
train_B_total <- colSums(train_B[folds!=j, ][-1])
val_set <- rbind(train_A[folds == j, ], train_B[folds == j, ])
posterior_prob <- dmm_model(param_val[i], train_A_total, train_B_total, val_set[-1])
val_identity <- as.numeric(val_set$author == "AaronPressman")
print(val_set)
# Compute R^2 for predicted values
cv_log_loss[i] = cv_log_loss[i] + log_loss(val_identity, posterior_prob)
}
# Average R^2 across k folds
cv_log_loss[i] = cv_log_loss[i] / k
}
# Return cross-validated R^2 values
return(cv_log_loss)
}
alphas <- c(1.)
dmm_cv_scores <- crossval_dmm(dataset1_train, alphas, 2)
crossval_dmm = function(data, param_val, k) {
# Input:
#   Training data frame: 'train',
#   Vector of span parameter values: 'param_val',
#   Number of CV folds: 'k'
# Output:
#   Vector of R^2 values for the provided parameters: 'cv_rsq'
train_A <- data[data$author == "AaronPressman", ]
train_B <- data[data$author != "AaronPressman", ]
num_param = length(param_val) # Number of parameters
set.seed(109) # Set seed for random number generator
# Divide training set into k folds by sampling uniformly at random
# folds[s] has the fold index for train instance 's'
folds = sample(1:k, nrow(train_A), replace = TRUE)
cv_log_loss = rep(0., num_param) # Store cross-validated R^2 for different parameter values
# Iterate over parameter values
for(i in 1:num_param){
# Iterate over folds to compute R^2 for parameter
for(j in 1:k){
# Fit model on all folds other than 'j' with parameter value param_val[i]
train_A_total <- colSums(train_A[folds!=j, ][-1])
train_B_total <- colSums(train_B[folds!=j, ][-1])
val_set <- rbind(train_A[folds == j, ], train_B[folds == j, ])
posterior_prob <- dmm_model(param_val[i], train_A_total, train_B_total, val_set[-1])
val_identity <- as.numeric(val_set$author == "AaronPressman")
print(val_identity)
# Compute R^2 for predicted values
cv_log_loss[i] = cv_log_loss[i] + log_loss(val_identity, posterior_prob)
}
# Average R^2 across k folds
cv_log_loss[i] = cv_log_loss[i] / k
}
# Return cross-validated R^2 values
return(cv_log_loss)
}
alphas <- c(1.)
dmm_cv_scores <- crossval_dmm(dataset1_train, alphas, 2)
crossval_dmm = function(data, param_val, k) {
# Input:
#   Training data frame: 'train',
#   Vector of span parameter values: 'param_val',
#   Number of CV folds: 'k'
# Output:
#   Vector of R^2 values for the provided parameters: 'cv_rsq'
train_A <- data[data$author == "AaronPressman", ]
train_B <- data[data$author != "AaronPressman", ]
num_param = length(param_val) # Number of parameters
set.seed(109) # Set seed for random number generator
# Divide training set into k folds by sampling uniformly at random
# folds[s] has the fold index for train instance 's'
folds = sample(1:k, nrow(train_A), replace = TRUE)
cv_log_loss = rep(0., num_param) # Store cross-validated R^2 for different parameter values
# Iterate over parameter values
for(i in 1:num_param){
# Iterate over folds to compute R^2 for parameter
for(j in 1:k){
# Fit model on all folds other than 'j' with parameter value param_val[i]
train_A_total <- colSums(train_A[folds!=j, ][-1])
train_B_total <- colSums(train_B[folds!=j, ][-1])
val_set <- rbind(train_A[folds == j, ], train_B[folds == j, ])
posterior_prob <- dmm_model(param_val[i], train_A_total, train_B_total, val_set[-1])
val_identity <- as.numeric(val_set$author == "AaronPressman")
print(posterior_prob>.5)
print(val_identity)
# Compute R^2 for predicted values
cv_log_loss[i] = cv_log_loss[i] + log_loss(val_identity, posterior_prob)
}
# Average R^2 across k folds
cv_log_loss[i] = cv_log_loss[i] / k
}
# Return cross-validated R^2 values
return(cv_log_loss)
}
alphas <- c(1.)
dmm_cv_scores <- crossval_dmm(dataset1_train, alphas, 2)
crossval_dmm = function(data, param_val, k) {
# Input:
#   Training data frame: 'train',
#   Vector of span parameter values: 'param_val',
#   Number of CV folds: 'k'
# Output:
#   Vector of R^2 values for the provided parameters: 'cv_rsq'
train_A <- data[data$author == "AaronPressman", ]
train_B <- data[data$author != "AaronPressman", ]
num_param = length(param_val) # Number of parameters
set.seed(109) # Set seed for random number generator
# Divide training set into k folds by sampling uniformly at random
# folds[s] has the fold index for train instance 's'
folds = sample(1:k, nrow(train_A), replace = TRUE)
cv_log_loss = rep(0., num_param) # Store cross-validated R^2 for different parameter values
# Iterate over parameter values
for(i in 1:num_param){
# Iterate over folds to compute R^2 for parameter
for(j in 1:k){
# Fit model on all folds other than 'j' with parameter value param_val[i]
train_A_total <- colSums(train_A[folds!=j, ][-1])
train_B_total <- colSums(train_B[folds!=j, ][-1])
val_set <- rbind(train_A[folds == j, ], train_B[folds == j, ])
posterior_prob <- dmm_model(param_val[i], train_A_total, train_B_total, val_set[-1])
val_identity <- as.numeric(val_set$author == "AaronPressman")
print(as.numeric(posterior_prob>.5))
print(val_identity)
# Compute R^2 for predicted values
cv_log_loss[i] = cv_log_loss[i] + log_loss(val_identity, posterior_prob)
}
# Average R^2 across k folds
cv_log_loss[i] = cv_log_loss[i] / k
}
# Return cross-validated R^2 values
return(cv_log_loss)
}
alphas <- c(1.)
dmm_cv_scores <- crossval_dmm(dataset1_train, alphas, 2)
log_loss <- function(author_identity, posterior_probability){
n <- length(author_identity)
log_loss_probability <- 0
for (i in 1:n) {
if (author_identity[i] == 1) {
log_loss_probability <- log_loss_probability + log(posterior_probability[i])
} else {
log_loss_probability <- log_loss_probability + log(1 - posterior_probability[i])
}
}
print(log_loss_probability)
print(n)
return(-1./n * log_loss_probability)
}
crossval_dmm = function(data, param_val, k) {
# Input:
#   Training data frame: 'train',
#   Vector of span parameter values: 'param_val',
#   Number of CV folds: 'k'
# Output:
#   Vector of R^2 values for the provided parameters: 'cv_rsq'
train_A <- data[data$author == "AaronPressman", ]
train_B <- data[data$author != "AaronPressman", ]
num_param = length(param_val) # Number of parameters
set.seed(109) # Set seed for random number generator
# Divide training set into k folds by sampling uniformly at random
# folds[s] has the fold index for train instance 's'
folds = sample(1:k, nrow(train_A), replace = TRUE)
cv_log_loss = rep(0., num_param) # Store cross-validated R^2 for different parameter values
# Iterate over parameter values
for(i in 1:num_param){
# Iterate over folds to compute R^2 for parameter
for(j in 1:k){
# Fit model on all folds other than 'j' with parameter value param_val[i]
train_A_total <- colSums(train_A[folds!=j, ][-1])
train_B_total <- colSums(train_B[folds!=j, ][-1])
val_set <- rbind(train_A[folds == j, ], train_B[folds == j, ])
posterior_prob <- dmm_model(param_val[i], train_A_total, train_B_total, val_set[-1])
val_identity <- as.numeric(val_set$author == "AaronPressman")
# Compute R^2 for predicted values
cv_log_loss[i] = cv_log_loss[i] + log_loss(val_identity, posterior_prob)
}
# Average R^2 across k folds
cv_log_loss[i] = cv_log_loss[i] / k
}
# Return cross-validated R^2 values
return(cv_log_loss)
}
alphas <- c(1.)
dmm_cv_scores <- crossval_dmm(dataset1_train, alphas, 2)
crossval_dmm = function(data, param_val, k) {
# Input:
#   Training data frame: 'train',
#   Vector of span parameter values: 'param_val',
#   Number of CV folds: 'k'
# Output:
#   Vector of R^2 values for the provided parameters: 'cv_rsq'
train_A <- data[data$author == "AaronPressman", ]
train_B <- data[data$author != "AaronPressman", ]
num_param = length(param_val) # Number of parameters
set.seed(109) # Set seed for random number generator
# Divide training set into k folds by sampling uniformly at random
# folds[s] has the fold index for train instance 's'
folds = sample(1:k, nrow(train_A), replace = TRUE)
cv_log_loss = rep(0., num_param) # Store cross-validated R^2 for different parameter values
# Iterate over parameter values
for(i in 1:num_param){
# Iterate over folds to compute R^2 for parameter
for(j in 1:k){
# Fit model on all folds other than 'j' with parameter value param_val[i]
train_A_total <- colSums(train_A[folds!=j, ][-1])
train_B_total <- colSums(train_B[folds!=j, ][-1])
val_set <- rbind(train_A[folds == j, ], train_B[folds == j, ])
posterior_prob <- dmm_model(param_val[i], train_A_total, train_B_total, val_set[-1])
val_identity <- as.numeric(val_set$author == "AaronPressman")
# Compute R^2 for predicted values
cv_log_loss[i] = cv_log_loss[i] + log_loss(val_identity, posterior_prob)
print(cv_log_loss[i])
}
# Average R^2 across k folds
cv_log_loss[i] = cv_log_loss[i] / k
}
# Return cross-validated R^2 values
return(cv_log_loss)
}
alphas <- c(1.)
dmm_cv_scores <- crossval_dmm(dataset1_train, alphas, 2)
posterior_prob_a1 <- dmm_model(1, author_A, author_B, author_til)
author_identity_test <- as.numeric(dataset1_test$author == "AaronPressman")
log_loss(author_identity_test, posterior_prob_a1)
log_loss <- function(author_identity, posterior_probability){
n <- length(author_identity)
log_loss_probability <- 0
for (i in 1:n) {
if (author_identity[i] == 1) {
log_loss_probability <- log_loss_probability + log(posterior_probability[i])
print(posterior_probability)
print(log(posterior_probability))
} else {
log_loss_probability <- log_loss_probability + log(1 - posterior_probability[i])
}
}
return(-1./n * log_loss_probability)
}
alphas <- c(1.)
dmm_cv_scores <- crossval_dmm(dataset1_train, alphas, 2)
posterior_prob_a1 <- dmm_model(1, author_A, author_B, author_til)
author_identity_test <- as.numeric(dataset1_test$author == "AaronPressman")
log_loss(author_identity_test, posterior_prob_a1)
library(e1071)
library(ggplot2)
words_preprocessed <- scan("words_preprocessed.txt", what = character(),sep = "")
dataset1_column_names <- c("author", words_preprocessed)
# Load training and testing data, passing the preprocessed words as column names
dataset1_train <- read.table("dataset1_train_processed_subset.txt", header = FALSE, sep = ",", col.names = dataset1_column_names)
dataset1_test <- read.table("dataset1_test_processed_subset.txt",header = FALSE, sep = ",", col.names = dataset1_column_names)
model.nb <- naiveBayes(author ~ ., data = dataset1_train)
pred.nb <- predict(model.nb, dataset1_test, type = "class")
table.nb <- table(pred.nb, dataset1_test$author)
accuracy.nb <- sum(diag(table.nb))/ sum(table.nb)
print(sprintf("Naive Bayes Model Accuracy: %.2f", accuracy.nb))
posterior_pA = function(alpha, yA = NULL, yB = NULL, y_til = NULL){
# number of features
K = length(yA)
# total word counts
n = sum(y_til)
nA = sum(yA)
nB = sum(yB)
# posterior predictive distribution of being class A
A1 = lfactorial(n) + lfactorial(nA) - lfactorial(n + nA)
A2 = sum(lfactorial(y_til + yA)) - sum(lfactorial(y_til)) - sum(lfactorial(yA))
A3 = lfactorial(n + nA) + lgamma(K*alpha) - lgamma(n + nA + K*alpha)
A4 = sum(lgamma(y_til + yA + alpha) - lfactorial(y_til + yA) - lgamma(alpha))
A5 = lfactorial(nB) + lgamma(K*alpha) - lgamma(nB + K*alpha)
A6 = sum(lgamma(yB + alpha) - lfactorial(yB) - lgamma(alpha))
R_A = exp(A1 + A2 + A3 + A4 + A5 + A6)
# posterior predictive distribution of being class B
B1 = lfactorial(n) + lfactorial(nB) - lfactorial(n + nB)
B2 = sum(lfactorial(y_til + yB)) - sum(lfactorial(y_til)) - sum(lfactorial(yB))
B3 = lfactorial(n + nB) + lgamma(K*alpha) - lgamma(n + nB + K*alpha)
B4 = sum(lgamma(y_til + yB + alpha) - lfactorial(y_til + yB) - lgamma(alpha))
B5 = lfactorial(nA) + lgamma(K*alpha) - lgamma(nA + K*alpha)
B6 = sum(lgamma(yA + alpha) - lfactorial(yA) - lgamma(alpha))
R_B = exp(B1 + B2 + B3 + B4 + B5 + B6)
# probability of being class A
pA = R_A/(R_A + R_B)
return(pA)
log_loss <- function(author_identity, posterior_probability){
n <- length(author_identity)
log_loss_probability <- 0
for (i in 1:n) {
if (author_identity[i] == 1) {
log_loss_probability <- log_loss_probability + log(posterior_probability[i])
} else {
log_loss_probability <- log_loss_probability + log(1 - posterior_probability[i])
}
}
return(-1./n * log_loss_probability)
}
dmm_model <- function(alpha, train_a, train_b, test_set) {
n <- nrow(test_set)
posterior_prob = rep(0., n)
for (i in 1:n) {
posterior_prob[i] <- posterior_pA(alpha, train_a, train_b, test_set[i, ])
}
return(posterior_prob)
}
crossval_dmm = function(data, param_val, k) {
# Input:
#   Training data frame: 'train',
#   Vector of span parameter values: 'param_val',
#   Number of CV folds: 'k'
# Output:
#   Vector of R^2 values for the provided parameters: 'cv_rsq'
train_A <- data[data$author == "AaronPressman", ]
train_B <- data[data$author != "AaronPressman", ]
num_param = length(param_val) # Number of parameters
set.seed(109) # Set seed for random number generator
# Divide training set into k folds by sampling uniformly at random
# folds[s] has the fold index for train instance 's'
folds = sample(1:k, nrow(train_A), replace = TRUE)
cv_log_loss = rep(0., num_param) # Store cross-validated R^2 for different parameter values
# Iterate over parameter values
for(i in 1:num_param){
# Iterate over folds to compute R^2 for parameter
for(j in 1:k){
# Fit model on all folds other than 'j' with parameter value param_val[i]
train_A_total <- colSums(train_A[folds!=j, ][-1])
train_B_total <- colSums(train_B[folds!=j, ][-1])
val_set <- rbind(train_A[folds == j, ], train_B[folds == j, ])
posterior_prob <- dmm_model(param_val[i], train_A_total, train_B_total, val_set[-1])
val_identity <- as.numeric(val_set$author == "AaronPressman")
# Compute R^2 for predicted values
cv_log_loss[i] = cv_log_loss[i] + log_loss(val_identity, posterior_prob)
}
# Average R^2 across k folds
cv_log_loss[i] = cv_log_loss[i] / k
}
# Return cross-validated R^2 values
return(cv_log_loss)
}
author_A <- colSums(dataset1_train[dataset1_train$author == "AaronPressman", ][-1])
author_B <- colSums(dataset1_train[dataset1_train$author != "AaronPressman", ][-1])
author_til <- dataset1_test[-1]
posterior_prob_a1 <- dmm_model(1, author_A, author_B, author_til)
author_identity_test <- as.numeric(dataset1_test$author == "AaronPressman")
log_loss(author_identity_test, posterior_prob_a1)
log_loss <- function(author_identity, posterior_probability){
n <- length(author_identity)
log_loss_probability <- 0
for (i in 1:n) {
if (author_identity[i] == 1) {
log_loss_probability <- log_loss_probability + log(posterior_probability[i])
print(posterior_probability[i])
} else {
log_loss_probability <- log_loss_probability + log(1 - posterior_probability[i])
}
}
return(-1./n * log_loss_probability)
}
posterior_prob_a1 <- dmm_model(1, author_A, author_B, author_til)
author_identity_test <- as.numeric(dataset1_test$author == "AaronPressman")
log_loss(author_identity_test, posterior_prob_a1)
author_A <- colSums(dataset1_train[dataset1_train$author == "AaronPressman", ][-1])
author_B <- colSums(dataset1_train[dataset1_train$author != "AaronPressman", ][-1])
author_til <- dataset1_test[-1]
posterior_prob_a1 <- dmm_model(1, author_A, author_B, author_til)
author_identity_test <- as.numeric(dataset1_test$author == "AaronPressman")
log_loss(author_identity_test, posterior_prob_a1)
log_loss <- function(author_identity, posterior_probability){
n <- length(author_identity)
log_loss_probability <- 0
for (i in 1:n) {
if (author_identity[i] == 1) {
log_loss_probability <- log_loss_probability + log(posterior_probability[i])
print(posterior_probability[i])
} else {
log_loss_probability <- log_loss_probability + log(1 - posterior_probability[i])
print(posterior_probability[i])
}
}
return(-1./n * log_loss_probability)
}
posterior_prob_a1 <- dmm_model(1, author_A, author_B, author_til)
author_identity_test <- as.numeric(dataset1_test$author == "AaronPressman")
log_loss(author_identity_test, posterior_prob_a1)
posterior_pA = function(alpha, yA = NULL, yB = NULL, y_til = NULL){
# number of features
K = length(yA)
# total word counts
n = sum(y_til)
nA = sum(yA)
nB = sum(yB)
# posterior predictive distribution of being class A
A1 = lfactorial(n) + lfactorial(nA) - lfactorial(n + nA)
A2 = sum(lfactorial(y_til + yA)) - sum(lfactorial(y_til)) - sum(lfactorial(yA))
A3 = lfactorial(n + nA) + lgamma(K*alpha) - lgamma(n + nA + K*alpha)
A4 = sum(lgamma(y_til + yA + alpha) - lfactorial(y_til + yA) - lgamma(alpha))
A5 = lfactorial(nB) + lgamma(K*alpha) - lgamma(nB + K*alpha)
A6 = sum(lgamma(yB + alpha) - lfactorial(yB) - lgamma(alpha))
R_A = exp(A1 + A2 + A3 + A4 + A5 + A6)
# posterior predictive distribution of being class B
B1 = lfactorial(n) + lfactorial(nB) - lfactorial(n + nB)
B2 = sum(lfactorial(y_til + yB)) - sum(lfactorial(y_til)) - sum(lfactorial(yB))
B3 = lfactorial(n + nB) + lgamma(K*alpha) - lgamma(n + nB + K*alpha)
B4 = sum(lgamma(y_til + yB + alpha) - lfactorial(y_til + yB) - lgamma(alpha))
B5 = lfactorial(nA) + lgamma(K*alpha) - lgamma(nA + K*alpha)
B6 = sum(lgamma(yA + alpha) - lfactorial(yA) - lgamma(alpha))
R_B = exp(B1 + B2 + B3 + B4 + B5 + B6)
# probability of being class A
pA = R_A/(R_A + R_B)
return(pA)
}
log_loss <- function(author_identity, posterior_probability){
n <- length(author_identity)
log_loss_probability <- 0
for (i in 1:n) {
if (author_identity[i] == 1) {
log_loss_probability <- log_loss_probability + log(posterior_probability[i])
print(posterior_probability[i])
} else {
log_loss_probability <- log_loss_probability + log(1 - posterior_probability[i])
}
}
return(-1./n * log_loss_probability)
}
dmm_model <- function(alpha, train_a, train_b, test_set) {
n <- nrow(test_set)
posterior_prob = rep(0., n)
for (i in 1:n) {
posterior_prob[i] <- posterior_pA(alpha, train_a, train_b, test_set[i, ])
}
return(posterior_prob)
}
author_A <- colSums(dataset1_train[dataset1_train$author == "AaronPressman", ][-1])
author_B <- colSums(dataset1_train[dataset1_train$author != "AaronPressman", ][-1])
author_til <- dataset1_test[-1]
posterior_prob_a1 <- dmm_model(1, author_A, author_B, author_til)
author_identity_test <- as.numeric(dataset1_test$author == "AaronPressman")
log_loss(author_identity_test, posterior_prob_a1)
