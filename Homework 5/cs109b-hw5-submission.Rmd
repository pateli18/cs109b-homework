---
title: "CS109b-hw5-submission"
author: "Ihsaan Patel"
date: "March 26, 2017"
output: pdf_document
---

# Libraries & Helper Functions

```{r helper_functions}
library(e1071)
library(caret)
library(cluster)
library(mclust)
library(corrplot)
library(dbscan)
library(MASS)
library(ggplot2)
library(ggfortify)
library(NbClust)
library(factoextra)

rot90 <- function(x, n = 1){
  #Rotates 90 degrees (counterclockwise)
  r90 <- function(x){
    y <- matrix(rep(NA, prod(dim(x))), nrow = nrow(x))
    for(i in seq_len(nrow(x))) y[, i] <- rev(x[i, ])
    y
  }
  for(i in seq_len(n)) x <- r90(x)
  return(x)
}

plot.face = function(x,zlim=c(-1,1)) {
  #Plots Face given image vector x
  x = pmin(pmax(x,zlim[1]),zlim[2])
  cols = gray.colors(100)[100:1]
  image(rot90(matrix(x,nrow=250)[,250:1],3),col=cols,
        zlim=zlim,axes=FALSE)  
}

agnes.reformat<-function(x, k){
# x: Data matrix or frame, k: Number of clusters
  x.agnes = agnes(x,method="ward",stand=T)
  x.cluster = list(cluster=cutree(x.agnes,k=k))
  return(x.cluster)
}

diana.reformat<-function(x, k){
# x: Data matrix or frame, k: Number of clusters
  x.diana = diana(x,stand=T)
  x.cluster = list(cluster=cutree(x.diana,k=k))
  return(x.cluster)
}
```

# Problem 1: Face recognition

```{r load_data}
load("CS109b-hw5-dataset_1.Rdata")
```

## Top 5 Principal Components

```{r pca_transform}
pca_object = prcomp(imgs_train, scale = TRUE, center = TRUE)
pca_prop_of_variance <- pca_object$sdev^2/sum(pca_object$sdev^2)
print(sprintf("Variance explained by Top 5 Components: %.2f",sum(pca_prop_of_variance[1:5])))
```

```{r visualize_eigenfaces}
eigenfaces_5pc = pca_object$x[, 1:5] %*% t(pca_object$rotation[, 1:5])
print("Original Image 1")
plot.face(imgs_train[1, ])
print("Eigenface Image 1")
plot.face(eigenfaces_5pc[1, ])
print("Original Image 2")
plot.face(imgs_train[2, ])
print("Eigenface Image 2")
plot.face(eigenfaces_5pc[2, ])

```

The eigenfaces show how much of the facial image information is captured by the top 5 principal components, which appears to be substantial as it's possible to see the outlines of the head, eyes, nose, mouth as well as other facial features.

## Principal Components - 90% of Variation

```{r 90%_variation}
cumulative_pca_prop_of_variance <- cumsum(pca_prop_of_variance)
for (i in 1:length(cumulative_pca_prop_of_variance)) {
  if (cumulative_pca_prop_of_variance[i] >= .90) {
    pca_90_index = i
    break
  }
}
print(sprintf("Top PCs that contribute 90 percent of the data: %i", pca_90_index))
```

The number of identified PCs is substantially less than the total number of pixels in the images (109 vs. 62,500) which helps reduce the problem of hyperdimensionality in the dataset.

```{r compute_pc_scores}
train_pca_data = data.frame(labels_train, predict(pca_object, imgs_train)[, 1:pca_90_index])
test_pca_data = data.frame(labels_test, predict(pca_object, imgs_test)[, 1:pca_90_index])
```

## PCA SVM Model

```{r svm_train}
costs <- 10^c(-10:10)
gammas <- 10^c(-10:10)

img.svm <- tune(svm, labels_train ~ ., data = train_pca_data, ranges = list(cost = costs, gamma = gammas, kernel = 'radial'), tunecontrol = tune.control(cross = 5))
```

```{r svm_predict}
# Predict and Calculate Accuracy for the SVM
pred <- predict(img.svm$best.model, test_pca_data)
cm_svm <- confusionMatrix(table(pred, labels_test))
print(sprintf("SVM Accuracy: %.4f", cm_svm$overall[[1]]))

# Naive Classifier
naive_predictions <- sample(x = ("George_W_Bush"), size = length(labels_test), replace = TRUE)
u <- union(naive_predictions, labels_test)
cm_naive <- confusionMatrix(table(factor(naive_predictions, u), factor(labels_test, u)))
print(sprintf("Naive Classifier: %.4f", cm_naive$overall[[1]]))
```

The SVM performs significantly better than a naiive classifier that just uses the dominant class, indicating that the PCA was still able to preserve the important predictive features while reducing dimensionality

# Problem 2: Analyzing Voting Patterns of US States

```{r load_dataset_2}
dataset_2 <- read.table("CS109b-hw5-dataset_2.txt", header = TRUE)
```

## Problem 2a: Visualize the data

```{r euclidea_distance}
euclidean_distance <- daisy(dataset_2, metric = "euclidean", stand = TRUE)
fviz_dist(euclidean_distance, gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))
```

```{r multi_dimensional_scaling}
multi_dimension_scaling <- cmdscale(euclidean_distance)
colnames(multi_dimension_scaling) <- c("var1", "var2")
ggplot(multi_dimension_scaling, aes(x=var1, y=var2)) + geom_point() +  ggtitle("Multi-Dimensional Scaling")
```

```{r pca_data}
pca_states <- prcomp(dataset_2, scale = TRUE, center = TRUE)
pca_states_data <- predict(pca_states, dataset_2)[, 1:2]
ggplot(pca_states_data, aes(x=PC1, y=PC2)) + geom_point() + geom_density2d() + ggtitle("PCA")
```

The visualizations show clustering around the middle of the data as well as the edges, which makes sense given the two party system anad the tendency for some states to consistently vote Democratic, others to consistently vote Republican, and other swing states that flip between Democratic and Republican depending on the election year. Interstingly the PCA plot appears to be a mirror image of the multi-dimensional scaling plot. There are no clear clusters and the density overlay only shows one center, but there looks to be around 4 to 5 based on visual inspection.

## Problem 2b: Partitioning Clustering

### KMeans

```{r k_means_gap}
kmeans_gapstat <- clusGap(scale(dataset_2),FUN=kmeans, K.max=10)
print(kmeans_gapstat, method = "Tibs2001SEmax")
```

Optimal number of methods is **1** as this satisfies the Tibsherani criteria

```{r k_means_elbow}
fviz_nbclust(scale(dataset_2), kmeans, method="gap_stat", k.max = 10) + ggtitle("K-means clustering for US Voting Patterns - optimal number of clusters")
```

Optimal number of methods is **1** as displayed in the chart

```{r k_means_silouhette}
fviz_nbclust(scale(dataset_2), kmeans, method="silhouette", k.max = 10) + ggtitle("K-means clustering for US Voting Patterns - optimal number of clusters")
```

Optimal number of clusters is **7** as displayed in the chart

**Summary**: The methods do not agree on the optimal number of clusters because the computation for each is different and the lack of clear clusters in the data visualizations means that these different computation methods can lead to different answers.

### PAM

```{r pam_gap}
pam_gapstat <- clusGap(scale(dataset_2),FUN=pam, K.max=10)
print(pam_gapstat, method = "Tibs2001SEmax")
```

Optimal number of methods is **1** as this satisfies the Tibsherani criteria

```{r pam_elbow}
fviz_nbclust(scale(dataset_2), pam, method="gap_stat", k.max = 10) + ggtitle("PAM clustering for US Voting Patterns - optimal number of clusters")
```

Optimal number of methods is **1** as displayed in the chart

```{r k_means_silouhette}
fviz_nbclust(scale(dataset_2), pam, method="silhouette", k.max = 10) + ggtitle("PAM clustering for US Voting Patterns - optimal number of clusters")
```

Optimal number of clusters is **3** as displayed in the chart

**Summary**: The methods do not agree on the optimal number of clusters because the computation for each is different and the lack of clear clusters in the data visualizations means that these different computation methods can lead to different answers.

### Principal Components Plot

```{r principal_components_plot}
optimal_clusters = 3
chosen_kmeans = kmeans(scale(dataset_2), optimal_clusters)
chosen_pam = pam(scale(dataset_2), optimal_clusters)
fviz_cluster(chosen_kmeans, data = scale(dataset_2), main = "KMeans Clustering of US Voting Data")
fviz_cluster(chosen_pam, main = "PAM Clustering of US Voting Data")
```

The clusterings are similar but not exactly the same, with the difference being in those datapoints towards the centers of both clusters. What's interesting is that the top and right clusters for both are clearly Republican (with the exception of Virginia in recent elections), whereas the left cluster includes Democratic states along with more conservative ones like Texas (with the more conservative ones very close to the other clusters).

### Silhouette Plots

```{r silhouette_plots}
# kmeans silhouette
fviz_silhouette(silhouette(chosen_kmeans$cluster, dist(scale(dataset_2))), main="Silhouette plot for KMeans clustering")
kmeans_sil = silhouette(chosen_kmeans$cluster, dist(scale(dataset_2)))[, 3]
kmeans_neg_sil_index = which(kmeans_sil < 0)
print("KMeans Misclassified States:")
print(row.names(dataset_2)[kmeans_neg_sil_index])

# PAM silhouette
fviz_silhouette(silhouette(chosen_pam), main="Silhouette plot for PAM clustering")
pam_sil = silhouette(chosen_pam)[, 3]
pam_neg_sil_index = which(pam_sil < 0)
print("PAM Misclassified States:")
print(row.names(dataset_2)[pam_neg_sil_index])
```

## Problem 2c: Hierarchical Clustering

### Agglomerative clustering

```{r agnes_dendogram}
pltree(agnes(dataset_2))
```


```{r agnes}
agnes_gapstat <- clusGap(scale(dataset_2),FUN=agnes.reformat, K.max=10)
print(agnes_gapstat, method = "Tibs2001SEmax")
```

While the optimal number of clusters is **1** using the Tibshirani metric, I'll instead use 2 (which is the next number to satisfy the metric) to visualize on the dendogram.

```{r dendogram_rectangle}
agnes_chosen = agnes(scale(dataset_2),method="ward",stand=T)
pltree(agnes(dataset_2))
rect.hclust(agnes_chosen, k = 2)
```

While the optimal cluster as chosen based on the gap statistic does not necessarily clearly delineate between traditionally Democratic and Republican voting states, the right box does lean heavily Democratic with Texas and Missippi the notable exceptions. The left box has mostly Republican voting states with some Democratic stalwarts like California.

```{r agnes_silhouette}
agnes_output <- as.integer((agnes.reformat(scale(dataset_2), 2))[[1]])
fviz_silhouette(silhouette(agnes_output, dist(scale(dataset_2))), main="Silhouette plot for AGNES clustering")
agnes_sil <- silhouette(agnes_output, dist(scale(dataset_2)))[, 3]
agnes_neg_sil_index <- which(agnes_sil < 0)
print("Agnes Misclassified States:")
print(row.names(dataset_2)[agnes_neg_sil_index])
```

The silhouette plot shows that 4 states look liklely to be misclassified.

### Divisive Clustering

```{r diana_dendogram}
pltree(diana(dataset_2))
```


```{r diana_gapstat}
diana_gapstat <- clusGap(scale(dataset_2),FUN=diana.reformat, K.max=10)
print(diana_gapstat, method = "Tibs2001SEmax")
```

While the optimal number of clusters is **1** using the Tibshirani metric, I'll instead use 3 (which is the next number to satisfy the metric) to visualize on the dendogram.

```{r diana_dendogram_rectangle}
diana_chosen = diana(scale(dataset_2),stand=T)
pltree(diana(dataset_2))
rect.hclust(diana_chosen, k = 3)
```

While the optimal cluster as chosen based on the gap statistic does not necessarily clearly delineate between traditionally Democratic and Republican voting states, the right box does lean heavily Democratic with Texas, Kentucky, Georgia and Louisiana the notable exceptions. The middle and left boxes have essentially all Republican leaning states

```{r diana_silhouette}
diana_output <- as.integer((diana.reformat(scale(dataset_2), 2))[[1]])
fviz_silhouette(silhouette(diana_output, dist(scale(dataset_2))), main="Silhouette plot for AGNES clustering")
diana_sil <- silhouette(diana_output, dist(scale(dataset_2)))[, 3]
diana_neg_sil_index <- which(diana_sil < 0)
print("Agnes Misclassified States:")
print(row.names(dataset_2)[diana_neg_sil_index])
```

The silhouette plot shows that all of the states look to be well classified with no negative silhouettes.

### Principal Components Plots

```{r agnes_pc_plot}
grp.agnes = cutree(agnes_chosen, k=2)
fviz_cluster(list(data = scale(dataset_2), cluster = grp.agnes), main="AGNES fit - 2 clusters")
```

```{r diana_pc_plot}
grp.diana = cutree(diana_chosen, k=3)
fviz_cluster(list(data = scale(dataset_2), cluster = grp.diana), main="DIANA fit - 2 clusters")
```

While the AGNES plot looks markedly different as it is only 2 clusters, it's left cluster is similar to the left and middle clusters in problem 2b. The right cluster is also similar though smaller as it does not include Virgina for example. The DIANA plot is very similar to the plots in problem 2b though the middle cluster is much smaller and only includes 4 states.

