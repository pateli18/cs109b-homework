---
title: "CS109b-hw5-submission"
author: "Ihsaan Patel"
date: "March 26, 2017"
output: pdf_document
---

# Libraries & Helper Functions

```{r helper_functions}
library(e1071)
library(caret)
library(cluster)
library(mclust)
library(corrplot)
library(dbscan)
library(MASS)
library(ggplot2)
library(ggfortify)
library(NbClust)
library(factoextra)

rot90 <- function(x, n = 1){
  #Rotates 90 degrees (counterclockwise)
  r90 <- function(x){
    y <- matrix(rep(NA, prod(dim(x))), nrow = nrow(x))
    for(i in seq_len(nrow(x))) y[, i] <- rev(x[i, ])
    y
  }
  for(i in seq_len(n)) x <- r90(x)
  return(x)
}

plot.face = function(x,zlim=c(-1,1)) {
  #Plots Face given image vector x
  x = pmin(pmax(x,zlim[1]),zlim[2])
  cols = gray.colors(100)[100:1]
  image(rot90(matrix(x,nrow=250)[,250:1],3),col=cols,
        zlim=zlim,axes=FALSE)  
}
```

# Problem 1: Face recognition

```{r load_data}
load("CS109b-hw5-dataset_1.Rdata")
```

## Top 5 Principal Components

```{r pca_transform}
pca_object = prcomp(imgs_train, scale = TRUE, center = TRUE)
pca_prop_of_variance <- pca_object$sdev^2/sum(pca_object$sdev^2)
print(sprintf("Variance explained by Top 5 Components: %.2f",sum(pca_prop_of_variance[1:5])))
```

```{r visualize_eigenfaces}
eigenfaces_5pc = pca_object$x[, 1:5] %*% t(pca_object$rotation[, 1:5])
print("Original Image 1")
plot.face(imgs_train[1, ])
print("Eigenface Image 1")
plot.face(eigenfaces_5pc[1, ])
print("Original Image 2")
plot.face(imgs_train[2, ])
print("Eigenface Image 2")
plot.face(eigenfaces_5pc[2, ])

```

The eigenfaces show how much of the facial image information is captured by the top 5 principal components, which appears to be substantial as it's possible to see the outlines of the head, eyes, nose, mouth as well as other facial features.

## Principal Components - 90% of Variation

```{r 90%_variation}
cumulative_pca_prop_of_variance <- cumsum(pca_prop_of_variance)
for (i in 1:length(cumulative_pca_prop_of_variance)) {
  if (cumulative_pca_prop_of_variance[i] >= .90) {
    pca_90_index = i
    break
  }
}
print(sprintf("Top PCs that contribute 90 percent of the data: %i", pca_90_index))
```

The number of identified PCs is substantially less than the total number of pixels in the images (109 vs. 62,500) which helps reduce the problem of hyperdimensionality in the dataset.

```{r compute_pc_scores}
train_pca_data = data.frame(labels_train, predict(pca_object, imgs_train)[, 1:pca_90_index])
test_pca_data = data.frame(labels_test, predict(pca_object, imgs_test)[, 1:pca_90_index])
```

## PCA SVM Model

```{r svm_train}
costs <- 10^c(-10:10)
gammas <- 10^c(-10:10)

img.svm <- tune(svm, labels_train ~ ., data = train_pca_data, ranges = list(cost = costs, gamma = gammas, kernel = 'radial'), tunecontrol = tune.control(cross = 5))
```

```{r svm_predict}
# Predict and Calculate Accuracy for the SVM
pred <- predict(img.svm$best.model, test_pca_data)
cm_svm <- confusionMatrix(table(pred, labels_test))
print(sprintf("SVM Accuracy: %.4f", cm_svm$overall[[1]]))

# Naive Classifier
naive_predictions <- sample(x = ("George_W_Bush"), size = length(labels_test), replace = TRUE)
u <- union(naive_predictions, labels_test)
cm_naive <- confusionMatrix(table(factor(naive_predictions, u), factor(labels_test, u)))
print(sprintf("Naive Classifier: %.4f", cm_naive$overall[[1]]))
```

The SVM performs significantly better than a naiive classifier that just uses the dominant class, indicating that the PCA was still able to preserve the important predictive features while reducing dimensionality

# Problem 2: Analyzing Voting Patterns of US States

```{r load_dataset_2}
dataset_2 <- read.table("CS109b-hw5-dataset_2.txt", header = TRUE)
```

## Problem 2a: Visualize the data

```{r euclidea_distance}
euclidean_distance <- daisy(dataset_2, metric = "euclidean", stand = TRUE)
fviz_dist(euclidean_distance, gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))
```

```{r multi_dimensional_scaling}
multi_dimension_scaling <- cmdscale(euclidean_distance)
colnames(multi_dimension_scaling) <- c("var1", "var2")
ggplot(multi_dimension_scaling, aes(x=var1, y=var2)) + geom_point() +  ggtitle("Multi-Dimensional Scaling")
```

```{r pca_data}
pca_states <- prcomp(dataset_2, scale = TRUE, center = TRUE)
pca_states_data <- predict(pca_states, dataset_2)[, 1:2]
ggplot(pca_states_data, aes(x=PC1, y=PC2)) + geom_point() + geom_density2d() + ggtitle("PCA")
```

The visualizations show clustering around the middle of the data as well as the edges, which makes sense given the two party system anad the tendency for some states to consistently vote Democratic, others to consistently vote Republican, and other swing states that flip between Democratic and Republican depending on the election year. Interstingly the PCA plot appears to be a mirror image of the multi-dimensional scaling plot. There are no clear clusters and the density overlay only shows one center, but there looks to be around 4 to 5 based on visual inspection.

## Problem 2b: Partitioning Clustering

### KMeans

```{r k_means_gap}
kmeans_gapstat <- clusGap(scale(dataset_2),FUN=kmeans, K.max=10)
print(kmeans_gapstat, method = "Tibs2001SEmax")
```

Optimal number of methods is **1** as this satisfies the Tibsherani criteria

```{r k_means_elbow}
fviz_nbclust(scale(dataset_2), kmeans, method="gap_stat", k.max = 10) + ggtitle("K-means clustering for US Voting Patterns - optimal number of clusters")
```

Optimal number of methods is **1** as displayed in the chart

```{r k_means_silouhette}
fviz_nbclust(scale(dataset_2), kmeans, method="silhouette", k.max = 10) + ggtitle("K-means clustering for US Voting Patterns - optimal number of clusters")
```

Optimal number of clusters is **7** as displayed in the chart

**Summary**: The methods do not agree on the optimal number of clusters because the computation for each is different and the lack of clear clusters in the data visualizations means that these different computation methods can lead to different answers.

### PAM

```{r pam_gap}
pam_gapstat <- clusGap(scale(dataset_2),FUN=pam, K.max=10)
print(pam_gapstat, method = "Tibs2001SEmax")
```

Optimal number of methods is **1** as this satisfies the Tibsherani criteria

```{r pam_elbow}
fviz_nbclust(scale(dataset_2), pam, method="gap_stat", k.max = 10) + ggtitle("PAM clustering for US Voting Patterns - optimal number of clusters")
```

Optimal number of methods is **1** as displayed in the chart

```{r k_means_silouhette}
fviz_nbclust(scale(dataset_2), pam, method="silhouette", k.max = 10) + ggtitle("PAM clustering for US Voting Patterns - optimal number of clusters")
```

Optimal number of clusters is **3** as displayed in the chart

**Summary**: The methods do not agree on the optimal number of clusters because the computation for each is different and the lack of clear clusters in the data visualizations means that these different computation methods can lead to different answers.

### Principal Components Plot

```{r principal_components_plot}
optimal_clusters = 3
chosen_kmeans = kmeans(scale(dataset_2), optimal_clusters)
chosen_pam = pam(scale(dataset_2), optimal_clusters)
fviz_cluster(chosen_kmeans, data = scale(dataset_2), main = "KMeans Clustering of US Voting Data")
fviz_cluster(chosen_pam, main = "PAM Clustering of US Voting Data")
```

The clusterings are similar but not exactly the same, with the difference being in those datapoints towards the centers of both clusters. What's interesting is that the top and right clusters for both are clearly Republican (with the exception of Virginia in recent elections), whereas the left cluster includes Democratic states along with more conservative ones like Texas (with the more conservative ones very close to the other clusters).

### Silhouette Plots

```{r silhouette_plots}
# kmeans silhouette
fviz_silhouette(silhouette(chosen_kmeans$cluster, dist(scale(dataset_2))), main="Silhouette plot for KMeans clustering")
kmeans_sil = silhouette(chosen_kmeans$cluster, dist(scale(dataset_2)))[, 3]
kmeans_neg_sil_index = which(kmeans_sil < 0)
print("KMeans Misclassified States:")
print(row.names(dataset_2)[kmeans_neg_sil_index])

# PAM silhouette
fviz_silhouette(silhouette(chosen_pam), main="Silhouette plot for PAM clustering")
pam_sil = silhouette(chosen_pam)[, 3]
pam_neg_sil_index = which(pam_sil < 0)
print("PAM Misclassified States:")
print(row.names(dataset_2)[pam_neg_sil_index])
```

