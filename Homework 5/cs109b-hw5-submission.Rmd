---
title: "CS109b-hw5-submission"
author: "Ihsaan Patel"
date: "March 26, 2017"
output: pdf_document
---

# Libraries & Helper Functions

```{r helper_functions}
library(e1071)
library(caret)

rot90 <- function(x, n = 1){
  #Rotates 90 degrees (counterclockwise)
  r90 <- function(x){
    y <- matrix(rep(NA, prod(dim(x))), nrow = nrow(x))
    for(i in seq_len(nrow(x))) y[, i] <- rev(x[i, ])
    y
  }
  for(i in seq_len(n)) x <- r90(x)
  return(x)
}

plot.face = function(x,zlim=c(-1,1)) {
  #Plots Face given image vector x
  x = pmin(pmax(x,zlim[1]),zlim[2])
  cols = gray.colors(100)[100:1]
  image(rot90(matrix(x,nrow=250)[,250:1],3),col=cols,
        zlim=zlim,axes=FALSE)  
}
```

# Problem 1: Face recognition

```{r load_data}
load("CS109b-hw5-dataset_1.Rdata")
```

## Top 5 Principal Components

```{r pca_transform}
pca_object = prcomp(imgs_train, scale = TRUE, center = TRUE)
pca_prop_of_variance <- pca_object$sdev^2/sum(pca_object$sdev^2)
print(sprintf("Variance explained by Top 5 Components: %.2f",sum(pca_prop_of_variance[1:5])))
```

```{r visualize_eigenfaces}
eigenfaces_5pc = pca_object$x[, 1:5] %*% t(pca_object$rotation[, 1:5])
print("Original Image 1")
plot.face(imgs_train[1, ])
print("Eigenface Image 1")
plot.face(eigenfaces_5pc[1, ])
print("Original Image 2")
plot.face(imgs_train[2, ])
print("Eigenface Image 2")
plot.face(eigenfaces_5pc[2, ])

```

The eigenfaces show how much of the facial image information is captured by the top 5 principal components, which appears to be substantial as it's possible to see the outlines of the head, eyes, nose, mouth as well as other facial features.

## Principal Components - 90% of Variation

```{r 90%_variation}
cumulative_pca_prop_of_variance <- cumsum(pca_prop_of_variance)
for (i in 1:length(cumulative_pca_prop_of_variance)) {
  if (cumulative_pca_prop_of_variance[i] >= .90) {
    pca_90_index = i
    break
  }
}
print(sprintf("Top PCs that contribute 90 percent of the data: %i", pca_90_index))
```

The number of identified PCs is substantially less than the total number of pixels in the images (109 vs. 62,500) which helps reduce the problem of hyperdimensionality in the dataset.

```{r compute_pc_scores}
train_pca_data = data.frame(labels_train, predict(pca_object, imgs_train)[, 1:pca_90_index])
test_pca_data = data.frame(labels_test, predict(pca_object, imgs_test)[, 1:pca_90_index])
```

## PCA SVM Model

```{r svm_train}
costs <- 10^c(-10:10)
gammas <- 10^c(-10:10)

img.svm <- tune(svm, labels_train ~ ., data = train_pca_data, ranges = list(cost = costs, gamma = gammas, kernel = 'radial'), tunecontrol = tune.control(cross = 5))
```

```{r svm_predict}
# Predict and Calculate Accuracy for the SVM
pred <- predict(img.svm$best.model, test_pca_data)
cm_svm <- confusionMatrix(table(pred, labels_test))
print(sprintf("SVM Accuracy: %.4f", cm_svm$overall[[1]]))

# Naive Classifier
naive_predictions <- sample(x = ("George_W_Bush"), size = length(labels_test), replace = TRUE)
u <- union(naive_predictions, labels_test)
cm_naive <- confusionMatrix(table(factor(naive_predictions, u), factor(labels_test, u)))
print(sprintf("Naive Classifier: %.4f", cm_naive$overall[[1]]))
```

The SVM performs significantly better than a naiive classifier that just uses the dominant class, indicating that the PCA was still able to preserve the important predictive features while reducing dimensionality

# Problem 2: