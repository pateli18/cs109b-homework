?poly
train_dataset2 <- read.table("dataset_2_train.txt",header = TRUE)
test_dataset2 <- read.table("dataset_2_test.txt",header = TRUE)
model.linear <- lm(ViolentCrimesPerPop ~ Population + PercentageBlack + PercentageWhite + PercentageAsian + PercentageHispanic + PercentageUrban + MedIncome, data = train_dataset2)
model.linear.test_predictions <- predict(model.linear, newdata = test_dataset2)
model.linear.test_r2 <- rsq(test_dataset2$ViolentCrimesPerPop, model.linear.test_predictions)
library(ggplot2)
# Function to compute R^2 for observed and predicted responses
rsq = function(y, predict) {
tss = sum((y - mean(y))^2)
rss = sum((y-predict)^2)
r_squared = 1 - rss/tss
return(r_squared)
}
crossval_ns = function(train, param_val, k) {
# Input:
#   Training data frame: 'train',
#   Vector of span parameter values: 'param_val',
#   Number of CV folds: 'k'
# Output:
#   Vector of R^2 values for the provided parameters: 'cv_rsq'
num_param = length(param_val) # Number of parameters
set.seed(109) # Set seed for random number generator
# Divide training set into k folds by sampling uniformly at random
# folds[s] has the fold index for train instance 's'
folds = sample(1:k, nrow(train), replace = TRUE)
cv_rsq = rep(0., num_param) # Store cross-validated R^2 for different parameter values
# Iterate over parameter values
for(i in 1:num_param){
# Iterate over folds to compute R^2 for parameter
for(j in 1:k){
# Fit model on all folds other than 'j' with parameter value param_val[i]
model.ns = lm(PickupCount ~ ns(TimeMin, df = param_val[i]), data = train[folds!=j, ])
# Make prediction on fold 'j'
pred = predict(model.ns, train[folds == j,])
# Compute R^2 for predicted values
cv_rsq[i] = cv_rsq[i] + rsq(train$PickupCount[folds == j], pred)
}
# Average R^2 across k folds
cv_rsq[i] = cv_rsq[i] / k
}
# Return cross-validated R^2 values
return(cv_rsq)
}
crossval_loess = function(train, param_val, k) {
# Input:
#   Training data frame: 'train',
#   Vector of span parameter values: 'param_val',
#   Number of CV folds: 'k'
# Output:
#   Vector of R^2 values for the provided parameters: 'cv_rsq'
num_param = length(param_val) # Number of parameters
set.seed(109) # Set seed for random number generator
# Divide training set into k folds by sampling uniformly at random
# folds[s] has the fold index for train instance 's'
folds = sample(1:k, nrow(train), replace = TRUE)
cv_rsq = rep(0., num_param) # Store cross-validated R^2 for different parameter values
# Iterate over parameter values
for(i in 1:num_param){
# Iterate over folds to compute R^2 for parameter
for(j in 1:k){
# Fit model on all folds other than 'j' with parameter value param_val[i]
model.loess = loess(PickupCount ~ TimeMin, span = param_val[i],
data = train[folds!=j, ],
control = loess.control(surface="direct"))
# Make prediction on fold 'j'
pred = predict(model.loess, train$TimeMin[folds == j])
# Compute R^2 for predicted values
cv_rsq[i] = cv_rsq[i] + rsq(train$PickupCount[folds == j], pred)
}
# Average R^2 across k folds
cv_rsq[i] = cv_rsq[i] / k
}
# Return cross-validated R^2 values
return(cv_rsq)
}
library(gam)
crossval_gams = function(train, param_val, k) {
# Input:
#   Training data frame: 'train',
#   Vector of span parameter values: 'param_val',
#   Number of CV folds: 'k'
# Output:
#   Vector of R^2 values for the provided parameters: 'cv_rsq'
num_param = length(param_val) # Number of parameters
set.seed(109) # Set seed for random number generator
# Divide training set into k folds by sampling uniformly at random
# folds[s] has the fold index for train instance 's'
folds = sample(1:k, nrow(train), replace = TRUE)
cv_rsq = rep(0., num_param) # Store cross-validated R^2 for different parameter values
# Iterate over parameter values
for(i in 1:num_param){
# Iterate over folds to compute R^2 for parameter
for(j in 1:k){
# Fit model on all folds other than 'j' with parameter value param_val[i]
gam_formula <- as.formula(sprintf("ViolentCrimesPerPop ~ s(Population, spar=%1$f) + s(PercentageBlack, spar=%1$f) + s(PercentageWhite,spar=%1$f) + s(PercentageAsian, spar=%1$f) + s(PercentageHispanic, spar=%1$f) + s(PercentageUrban, spar=%1$f) + s(MedIncome, spar=%1$f)", param_val[i]))
model.gams = gam(gam_formula, data = train[folds!=j, ])
# Make prediction on fold 'j'
pred = predict(model.gams, train[folds == j, 2:8])
# Compute R^2 for predicted values
cv_rsq[i] = cv_rsq[i] + rsq(train$ViolentCrimesPerPop[folds == j], pred)
}
# Average R^2 across k folds
cv_rsq[i] = cv_rsq[i] / k
}
# Return cross-validated R^2 values
return(cv_rsq)
}
crossval_gams_lo1 = function(train, param_val, spar_param, k) {
# Input:
#   Training data frame: 'train',
#   Vector of span parameter values: 'param_val',
#   Number of CV folds: 'k'
# Output:
#   Vector of R^2 values for the provided parameters: 'cv_rsq'
num_param = length(param_val) # Number of parameters
set.seed(109) # Set seed for random number generator
# Divide training set into k folds by sampling uniformly at random
# folds[s] has the fold index for train instance 's'
folds = sample(1:k, nrow(train), replace = TRUE)
cv_rsq = rep(0., num_param) # Store cross-validated R^2 for different parameter values
# Iterate over parameter values
for(i in 1:num_param){
# Iterate over folds to compute R^2 for parameter
for(j in 1:k){
# Fit model on all folds other than 'j' with parameter value param_val[i]
gam_formula <- as.formula(sprintf("ViolentCrimesPerPop ~ s(Population, spar=%1$f) + s(PercentageBlack, spar=%1$f) + s(PercentageWhite,spar=%1$f) + s(PercentageAsian, spar=%1$f) + s(PercentageHispanic, spar=%1$f) + s(PercentageUrban, spar=%1$f) + s(MedIncome, spar=%1$f) + lo(Population*PercentageUrban*MedIncome, span=%2$f)", spar_param, param_val[i]))
model.gams = gam(gam_formula, data = train[folds!=j, ])
# Make prediction on fold 'j'
pred = predict(model.gams, train[folds == j, 2:8])
# Compute R^2 for predicted values
cv_rsq[i] = cv_rsq[i] + rsq(train$ViolentCrimesPerPop[folds == j], pred)
}
# Average R^2 across k folds
cv_rsq[i] = cv_rsq[i] / k
}
# Return cross-validated R^2 values
return(cv_rsq)
}
crossval_gams_lo2 = function(train, param_val, spar_param, k) {
# Input:
#   Training data frame: 'train',
#   Vector of span parameter values: 'param_val',
#   Number of CV folds: 'k'
# Output:
#   Vector of R^2 values for the provided parameters: 'cv_rsq'
num_param = length(param_val) # Number of parameters
set.seed(109) # Set seed for random number generator
# Divide training set into k folds by sampling uniformly at random
# folds[s] has the fold index for train instance 's'
folds = sample(1:k, nrow(train), replace = TRUE)
cv_rsq = rep(0., num_param) # Store cross-validated R^2 for different parameter values
# Iterate over parameter values
for(i in 1:num_param){
# Iterate over folds to compute R^2 for parameter
for(j in 1:k){
# Fit model on all folds other than 'j' with parameter value param_val[i]
gam_formula <- as.formula(sprintf("ViolentCrimesPerPop ~ s(Population, spar=%1$f) + s(PercentageBlack, spar=%1$f) + s(PercentageWhite,spar=%1$f) + s(PercentageAsian, spar=%1$f) + s(PercentageHispanic, spar=%1$f) + s(PercentageUrban, spar=%1$f) + s(MedIncome, spar=%1$f) + lo(PercentageWhite*MedIncome, span=%2$f)", spar_param, param_val[i]))
model.gams = gam(gam_formula, data = train[folds!=j, ])
# Make prediction on fold 'j'
pred = predict(model.gams, train[folds == j, 2:8])
# Compute R^2 for predicted values
cv_rsq[i] = cv_rsq[i] + rsq(train$ViolentCrimesPerPop[folds == j], pred)
}
# Average R^2 across k folds
cv_rsq[i] = cv_rsq[i] / k
}
# Return cross-validated R^2 values
return(cv_rsq)
}
# Function to plot and evaluate model
model_evaluate = function(model_name, model, train_data, test_data) {
if (class(model) == "smooth.spline") {
train_predictions <- predict(model, x = train_data$TimeMin)$y
test_predictions <- predict(model, newdata = test_data$TimeMin)$y
} else {
train_predictions <- predict(model, newdata = train_data)
test_predictions <- predict(model, newdata = test_data)
}
train_r2 <- rsq(train_data$PickupCount, train_predictions)
test_r2 <- rsq(test_data$PickupCount, test_predictions)
graph_title <- sprintf("%s | Train R^2: %.4f | Test R^2: %.4f", model_name, train_r2, test_r2)
ggplot(transform(train_data, pred = train_predictions), mapping = aes(x = TimeMin, y=PickupCount)) + geom_point() + geom_line(mapping = aes(y = pred), color = "blue", size = 2) + ggtitle(graph_title)
}
# Taken from: http://www.cookbook-r.com/Graphs/Multiple_graphs_on_one_page_(ggplot2)/
# Multiple plot function
#
# ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects)
# - cols:   Number of columns in layout
# - layout: A matrix specifying the layout. If present, 'cols' is ignored.
#
# If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE),
# then plot 1 will go in the upper left, 2 will go in the upper right, and
# 3 will go all the way across the bottom.
#
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
library(grid)
# Make a list from the ... arguments and plotlist
plots <- c(list(...), plotlist)
numPlots = length(plots)
# If layout is NULL, then use 'cols' to determine layout
if (is.null(layout)) {
# Make the panel
# ncol: Number of columns of plots
# nrow: Number of rows needed, calculated from # of cols
layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
ncol = cols, nrow = ceiling(numPlots/cols))
}
if (numPlots==1) {
print(plots[[1]])
} else {
# Set up the page
grid.newpage()
pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))
# Make each plot, in the correct location
for (i in 1:numPlots) {
# Get the i,j matrix positions of the regions that contain this subplot
matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))
print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
layout.pos.col = matchidx$col))
}
}
}
model.linear <- lm(ViolentCrimesPerPop ~ Population + PercentageBlack + PercentageWhite + PercentageAsian + PercentageHispanic + PercentageUrban + MedIncome, data = train_dataset2)
model.linear.test_predictions <- predict(model.linear, newdata = test_dataset2)
model.linear.test_r2 <- rsq(test_dataset2$ViolentCrimesPerPop, model.linear.test_predictions)
model.linear <- lm(ViolentCrimesPerPop ~ Population + PercentageBlack + PercentageWhite + PercentageAsian + PercentageHispanic + PercentageUrban + MedIncome, data = train_dataset2)
model.linear.test_predictions <- predict(model.linear, newdata = test_dataset2)
model.linear.test_r2 <- rsq(test_dataset2$ViolentCrimesPerPop, model.linear.test_predictions)
# Fit and Score Polynomial Degree - 2
model.poly2 <- lm(ViolentCrimesPerPop ~ poly(Population + PercentageBlack + PercentageWhite + PercentageAsian + PercentageHispanic + PercentageUrban + MedIncome, degree = 2, raw = TRUE), data = train_dataset2)
model.poly2.test_predictions <- predict(model.poly2, newdata = test_dataset2)
model.poly2.test_r2 <- rsq(test_dataset2$ViolentCrimesPerPop, model.poly2.test_predictions)
# Fit and Score Polynomial Degree - 3
model.poly3 <- lm(ViolentCrimesPerPop ~ poly(Population + PercentageBlack + PercentageWhite + PercentageAsian + PercentageHispanic + PercentageUrban + MedIncome, degree = 3, raw = TRUE), data = train_dataset2)
model.poly3.test_predictions <- predict(model.poly3, newdata = test_dataset2)
model.poly3.test_r2 <- rsq(test_dataset2$ViolentCrimesPerPop, model.poly3.test_predictions)
# Fit and Score BSplines
model.bs <- lm(ViolentCrimesPerPop ~ bs(Population + PercentageBlack + PercentageWhite + PercentageAsian + PercentageHispanic + PercentageUrban + MedIncome, df = 3), data = train_dataset2)
model.bs.test_predictions <- predict(model.bs, newdata = test_dataset2)
model.bs.test_r2 <- rsq(test_dataset2$ViolentCrimesPerPop, model.bs.test_predictions)
# Print Output
print(sprintf("Linear Model R^2: %.4f", model.linear.test_r2))
print(sprintf("Polynomial Degree 2 R^2: %.4f", model.poly2.test_r2))
print(sprintf("Polynomial Degree 3 R^2: %.4f", model.poly3.test_r2))
print(sprintf("Basis Splines R^2: %.4f", model.bs.test_r2))
summary(model.poly2)
print(summary(model.poly2))
model.poly2$coefficients
formula_poly = as.formula(paste0("ViolentCrimesPerPop ~ ", paste0(myvars[2:8], collapse="+"),
"+", paste0("poly(", myvars[2:8], "^2)",collapse="+")))
my_vars = colnames(train_dataset2)
formula_poly = as.formula(paste0("ViolentCrimesPerPop ~ ", paste0(myvars[2:8], collapse="+"), "+", paste0("poly(", myvars[2:8], "^2)",collapse="+")))
myvars = colnames(train_dataset2)
formula_poly = as.formula(paste0("ViolentCrimesPerPop ~ ", paste0(myvars[2:8], collapse="+"), "+", paste0("poly(", myvars[2:8], "^2)",collapse="+")))
model.poly = lm(formula_poly, data=train_dataset2)
model.poly$coefficients
poly2formula = as.formula("ViolentCrimesPerPop ~ poly(Population + PercentageBlack + PercentageWhite + PercentageAsian + PercentageHispanic + PercentageUrban + MedIncome, degree = 2, raw = TRUE)")
model.poly2 <- lm(poly2formula, data = train_dataset2)
model.poly2.test_predictions <- predict(model.poly2, newdata = test_dataset2)
model.poly2.test_r2 <- rsq(test_dataset2$ViolentCrimesPerPop, model.poly2.test_predictions)
print(sprintf("Linear Model R^2: %.4f", model.linear.test_r2))
print(sprintf("Polynomial Degree 2 R^2: %.4f", model.poly2.test_r2))
print(sprintf("Polynomial Degree 3 R^2: %.4f", model.poly3.test_r2))
print(sprintf("Basis Splines R^2: %.4f", model.bs.test_r2))
model.poly2$coefficients
poly2formula
formula_poly
myvars = colnames(train_dataset2)
formula_poly = as.formula(paste0("ViolentCrimesPerPop ~ ", paste0(myvars[2:8], collapse="+"), "+", paste0("poly(", myvars[2:8], "degree = 2)",collapse="+")))
model.poly = lm(formula_poly, data=train_dataset2)
formula_poly
myvars = colnames(train_dataset2)
formula_poly = as.formula(paste0("ViolentCrimesPerPop ~ ", paste0(myvars[2:8], collapse="+"), "+", paste0("poly(", myvars[2:8], ", degree = 2)",collapse="+")))
model.poly = lm(formula_poly, data=train_dataset2)
model.poly$coefficients
myvars = colnames(train_dataset2)
formula_poly = as.formula(paste0("ViolentCrimesPerPop ~ ", paste0("poly(", myvars[2:8], ", degree = 2)",collapse="+")))
model.poly = lm(formula_poly, data=train_dataset2)
model.poly$coefficients
myvars = colnames(train_dataset2)
formula_poly = as.formula(paste0("ViolentCrimesPerPop ~ ", paste0(myvars[2:8], collapse="+"), "+", paste0("poly(", myvars[2:8], "^2)",collapse="+")))
model.poly = lm(formula_poly, data=train_dataset2)
model.poly$coefficients
myvars = colnames(train_dataset2)
formula_poly = as.formula(paste0("ViolentCrimesPerPop ~ ", paste0("poly(", myvars[2:8], ", degree = 2, raw = TRUE)",collapse="+")))
model.poly = lm(formula_poly, data=train_dataset2)
model.poly$coefficients
myvars = colnames(train_dataset2)
formula_poly = as.formula(paste0("ViolentCrimesPerPop ~ ", paste0(myvars[2:8], collapse="+"), "+", paste0("poly(", myvars[2:8], "^2)",collapse="+")))
model.poly = lm(formula_poly, data=train_dataset2)
myvars = colnames(train_dataset2)
formula_poly = as.formula(paste0("ViolentCrimesPerPop ~ ", paste0(myvars[2:8], collapse="+"), "+", paste0("poly(", myvars[2:8], "^2)",collapse="+")))
model.poly = lm(formula_poly, data=train_dataset2)
formula_poly
