---
title: "CS109b-hw4-submission"
author: "Ihsaan Patel"
date: "March 4, 2017"
output: pdf_document
---

# Libraries

```{r libraries, message=FALSE}
library(e1071)
library(caret)
library(mclust)
library(MCMCpack)
```

# Problem 1: Celestial Object Classification

## Load Data

```{r load_datasets_1}
# Load training set
dataset_1_train <- read.table("datasets/dataset_1_train.txt", header = TRUE, sep = ",")
dataset_1_train$Class <- as.factor(dataset_1_train$Class)

# Load testing set
dataset_1_test <- read.table("datasets/dataset_1_test.txt", header = TRUE, sep = ",")
dataset_1_test$Class <- as.factor(dataset_1_test$Class)
```

## 1. RBF Kernel: Gamma = 1, Cost = 1

```{r rbf_g1_c1}
# Fit SVM
model.svm_rbf_g1_c1 <- svm(Class ~ ., data = dataset_1_train, cost = 1, gamma = 1, kernel = "radial")

# Predict Test Set and Calculate Misclassification Rate
misclassification_rate.svm_rbf_g1_c1 <- classError(predict(model.svm_rbf_g1_c1, dataset_1_test), dataset_1_test$Class)$errorRate
print(sprintf("SVM Class = 1 Gamma = 1 Misclassification Rate: %.4f", misclassification_rate.svm_rbf_g1_c1))
```

## 2. Confusion Matrix

### Train Confusion Matrix
```{r train_confusion_matrix}
# Print Train Confusion Matrix
confusionMatrix(table(predict(model.svm_rbf_g1_c1, dataset_1_train), dataset_1_train$Class))
```

### Test Confusion Matrix

```{r test_confusion_matrix}
# Print Test Confusion Matrix
confusionMatrix(table(predict(model.svm_rbf_g1_c1, dataset_1_test), dataset_1_test$Class))
```

The model appears to overfit the training set by correctly predicting the class for every observation, while for the testing set the model just predicts class 3 for every observation.

## 3. Tuning Gamma

```{r tune_gamma}
# Initiate list with gammas and lists to store error rates
gammas <- c(0.001, 0.005, 0.01, 0.05, 0.1, 0.2, 0.3)
gamma_train_scores <- rep(0., length(gammas))
gamma_test_scores <- rep(0., length(gammas))

# Loop through each gamma value, fitting an svm and calculating training and test error artes
for (gamma in 1:length(gammas)) {
  model.svm_tune_gamma <- svm(Class ~ ., data = dataset_1_train, cost = 1, gamma = gammas[gamma], kernel = "radial")
  gamma_train_scores[gamma] <- classError(predict(model.svm_tune_gamma, dataset_1_train), dataset_1_train$Class)$errorRate
  gamma_test_scores[gamma] <- classError(predict(model.svm_tune_gamma, dataset_1_test), dataset_1_test$Class)$errorRate
}

# Plot training and test error rates
gamma_scores <- data.frame(gammas = gammas, train_scores = gamma_train_scores, test_scores = gamma_test_scores)
ggplot(gamma_scores, aes(gammas)) + geom_line(aes(y = train_scores, colour = "Train Error Rate")) + geom_line(aes(y = test_scores, colour = "Test Error Rate")) + ylab("Error Rate") + xlab("Gamma Value") + ggtitle("SVM Error Rate by Gamma Parameter")
```

It looks like model performance on the test set improves at very low levels as gamma increases (until gamma = 0.01) but then deteriorates rapidly as gamma continues to increase. The train error rate continues to improve as gamma increases, indicating clear overfiting post gamma = 0.01.

## 4. Tuning Class

```{r tune_class}
# Initiate list with costs and lists to store error rates
costs <- c(0.1, 0.25, 0.5, 0.75, 1., 2.5, 5., 7.5, 10., 12.5, 15., 17.5, 20.)
cost_train_scores <- rep(0., length(costs))
cost_test_scores <- rep(0., length(costs))

# Loop through each cost value, fitting an svm and calculating training and test error artes
for (cost in 1:length(costs)) {
  model.svm_tune_cost <- svm(Class ~ ., data = dataset_1_train, cost = costs[cost], gamma = 0.01, kernel = "radial")
  cost_train_scores[cost] <- classError(predict(model.svm_tune_cost, dataset_1_train), dataset_1_train$Class)$errorRate
  cost_test_scores[cost] <- classError(predict(model.svm_tune_cost, dataset_1_test), dataset_1_test$Class)$errorRate
}

# Plot training and test error rates
cost_scores <- data.frame(costs = costs, train_scores = cost_train_scores, test_scores = cost_test_scores)
ggplot(cost_scores, aes(costs)) + geom_line(aes(y = train_scores, colour = "Train Error Rate")) + geom_line(aes(y = test_scores, colour = "Test Error Rate")) + ylab("Error Rate") + xlab("Cost Value") + ggtitle("SVM Error Rate by Cost Parameter")
```

It looks like model performance on the test set improves at low levels as cost increases (until cost = 2.5) but then deteriorates slowly as cost continues to increase. The train error rate continues to improve as cost increases, indicating clear overfiting post cost = 2.5.

## 5. Tune Various SVM Models

```{r tuning_parameters}
costs <- c(0.001, 0.01, 0.1, 1, 10, 20)
gammas <- c(0.001, 0.01, 0.1, 1, 10, 20)
```

### Linear Model

```{r tune_svm_linear_model}
# Tune SVM Linear Model
svm.linear.tune <- tune(svm, Class ~ ., data = dataset_1_train, ranges = list(cost = costs, kernel = 'linear'), tunecontrol = tune.control(cross = 5))

# Print tune output
svm.linear.tune

# Plot error rate over tuning parameters
ggplot(svm.linear.tune$performances, mapping = aes(x = cost, y = error)) + geom_line()
svm.linear.tune_error_rate <- classError(predict(svm.linear.tune$best.model, dataset_1_test), dataset_1_test$Class)$errorRate

# Print misclassification rate
sprintf("SVM Linear Misclassification Rate: %.4f", svm.linear.tune_error_rate)
```

### Poly Model

```{r tune_svm_poly_model}
# Tune SVM Poly Model
svm.poly.tune <- tune(svm, Class ~ ., data = dataset_1_train, ranges = list(gamma = gammas, cost = costs, kernel = 'polynomial', degree = 2), tunecontrol = tune.control(cross = 5))

# Print tune output
svm.poly.tune

# Plot error rate over tuning parameters
ggplot(svm.poly.tune$performances, mapping = aes(x = gamma, y = error)) + geom_line() + facet_wrap(~cost, labeller = label_both)

# Print misclassification rate
svm.poly.tune_error_rate <- classError(predict(svm.poly.tune$best.model, dataset_1_test), dataset_1_test$Class)$errorRate
sprintf("SVM Poly Misclassification Rate: %.4f", svm.poly.tune_error_rate)
```

### RBF Model

```{r tune_svm_rbf_model}
# Tune SVM RBF Model
svm.rbf.tune <- tune(svm, Class ~ ., data = dataset_1_train, ranges = list(gamma = gammas, cost = costs, kernel = 'radial'), tunecontrol = tune.control(cross = 5))

# Print tune output
svm.rbf.tune

# Plot error rate over tuning parameters
ggplot(svm.rbf.tune$performances, mapping = aes(x = gamma, y = error)) + geom_line() + facet_wrap(~cost, labeller = label_both)

# Print misclassification rate
svm.rbf.tune_error_rate <- classError(predict(svm.rbf.tune$best.model, dataset_1_test), dataset_1_test$Class)$errorRate
sprintf("SVM RBF Misclassification Rate: %.4f", svm.rbf.tune_error_rate)
```

## 6. Best Model

The linear is the best in terms of model accuracy as it has a lower misclassification rate on the test set when compared to the tuned polynomial and radial models. It also performs much better than a naive classifier that predicts the most common class on all points, which is what the original SVM RBF Model with cost = 1 and gamma = 1 also did.

# Problem 2: Return of the Bayesian Hierarchical Modedl

```{r load_dataset_2}
# Load dataset
dataset_2 <- read.table("datasets/dataset_2.txt", header = TRUE, sep = ",")
```

## 1(a) Pooled Model

```{r pooled_model}
model.pooled <- glm(contraceptive_use ~ living.children, data = dataset_2, family = binomial(link = "logit"))
summary(model.pooled)
```

The number of living children increases the probability that a women uses contraception.

## 1(b) Unpooled Model

```{r unpooled_model, warning=FALSE}
model.unpooled <- glm(contraceptive_use ~ -1 + living.children*as.factor(district), data = dataset_2, family = binomial(link = "logit"))
summary(model.unpooled)
```

This model formula creates an interaction term between each district and living children, and so each coefficient of the interaction terms represents an individual model for each district since the -1 ensures there is no intercept. It looks like the sign and magnitude of the coefficient for each district differs (though not all statistically significant), indicating that living children have different impacts on contraceptive use.

## 1(c) Bayesian Hierarchical Logistic Model

```{r bayesian_hiearchical_logistic_model}
model.hierarchical <- MCMChlogit(fixed = contraceptive_use ~ living.children, random = ~ living.children, group = "district", data = dataset_2, r = 2, R = diag(c(1, 0.1)), burnin = 5000, mcmc = 1000)
summary(model.hierarchical)
```

The model returns distributions of intercepts and living children coefficients for each district as well as the estimated probability for each district model. The pooled and unpooled model do not these return probabilities.

## 2(a) Plot living.children Coefficients

```{r plot_living_children_coefficients}
# Get list and number of districts
districts <- unique(dataset_2$district)
n_districts <- length(districts)

# Get pooled coefficients
pooled_coefficients <- rep(model.pooled$coefficients[2], n_districts)

# Get unpooled coeffficents
all_unpooled_coefficients <- model.unpooled$coefficient
n_all_unpooled_coefficients <- length(all_unpooled_coefficients)
unpooled_coefficients <- all_unpooled_coefficients[(n_all_unpooled_coefficients - n_districts + 2):n_all_unpooled_coefficients]
unpooled_coefficients <- c(0, unpooled_coefficients) + all_unpooled_coefficients[1]

# Get bayesian hierarchical model coefficients
all_hierarchical_coefficients <- summary(model.hierarchical$mcmc)$statistics[, 1]
n_all_hierarchical_coefficients <- length(all_hierarchical_coefficients)
hierarchical_coefficients <- all_hierarchical_coefficients[(n_all_hierarchical_coefficients - 5 - n_districts):(n_all_hierarchical_coefficients - 6)]

# Plot coefficients
coefficient_graph_data <- data.frame(districts = districts, pooled = pooled_coefficients, unpooled = unpooled_coefficients, hierarchical = hierarchical_coefficients)
ggplot(coefficient_graph_data, aes(districts)) + geom_point(aes(y = pooled, colour = "Pooled Coefficients")) + geom_point(aes(y = unpooled, colour = "Unpooled Coefficients")) + geom_point(aes(y = hierarchical, colour = "Bayesian Hierarchical Coefficients")) + ylab("Coefficient Value") + xlab("District") + ggtitle("Model Coefficient by District") + ylim(-2, 2)
```

Vertical axis set between -2 and 2, since any coefficients larger than those that have little practical significance and eliminating them increases visibility into the variance in coefficients in the bayesian hierarchical model.

## 2(b) Plot Summary