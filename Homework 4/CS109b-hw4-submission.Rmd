---
title: "CS109b-hw4-submission"
author: "Ihsaan Patel"
date: "March 4, 2017"
output: pdf_document
---

# Libraries

```{r libraries, message=FALSE}
library(e1071)
library(caret)
library(mclust)
library(MCMCpack)
```

# Problem 1: Celestial Object Classification

## Load Data

```{r load_datasets_1}
# Load training set
dataset_1_train <- read.table("datasets/dataset_1_train.txt", header = TRUE, sep = ",")
dataset_1_train$Class <- as.factor(dataset_1_train$Class)

# Load testing set
dataset_1_test <- read.table("datasets/dataset_1_test.txt", header = TRUE, sep = ",")
dataset_1_test$Class <- as.factor(dataset_1_test$Class)
```

## 1. RBF Kernel: Gamma = 1, Cost = 1

```{r rbf_g1_c1}
# Fit SVM
model.svm_rbf_g1_c1 <- svm(Class ~ ., data = dataset_1_train, cost = 1, gamma = 1, kernel = "radial")

# Predict Test Set and Calculate Misclassification Rate
misclassification_rate.svm_rbf_g1_c1 <- classError(predict(model.svm_rbf_g1_c1, dataset_1_test), dataset_1_test$Class)$errorRate
print(sprintf("SVM Class = 1 Gamma = 1 Misclassification Rate: %.4f", misclassification_rate.svm_rbf_g1_c1))
```

## 2. Confusion Matrix

### Train Confusion Matrix
```{r train_confusion_matrix}
# Print Train Confusion Matrix
confusionMatrix(table(predict(model.svm_rbf_g1_c1, dataset_1_train), dataset_1_train$Class))
```

### Test Confusion Matrix

```{r test_confusion_matrix}
# Print Test Confusion Matrix
confusionMatrix(table(predict(model.svm_rbf_g1_c1, dataset_1_test), dataset_1_test$Class))
```

The model appears to overfit the training set by correctly predicting the class for every observation, while for the testing set the model just predicts class 3 for every observation.

## 3. Tuning Gamma

```{r tune_gamma}
# Initiate list with gammas and lists to store error rates
gammas <- c(0.001, 0.005, 0.01, 0.05, 0.1, 0.2, 0.3)
gamma_train_scores <- rep(0., length(gammas))
gamma_test_scores <- rep(0., length(gammas))

# Loop through each gamma value, fitting an svm and calculating training and test error artes
for (gamma in 1:length(gammas)) {
  model.svm_tune_gamma <- svm(Class ~ ., data = dataset_1_train, cost = 1, gamma = gammas[gamma], kernel = "radial")
  gamma_train_scores[gamma] <- classError(predict(model.svm_tune_gamma, dataset_1_train), dataset_1_train$Class)$errorRate
  gamma_test_scores[gamma] <- classError(predict(model.svm_tune_gamma, dataset_1_test), dataset_1_test$Class)$errorRate
}

# Plot training and test error rates
gamma_scores <- data.frame(gammas = gammas, train_scores = gamma_train_scores, test_scores = gamma_test_scores)
ggplot(gamma_scores, aes(gammas)) + geom_line(aes(y = train_scores, colour = "Train Error Rate")) + geom_line(aes(y = test_scores, colour = "Test Error Rate")) + ylab("Error Rate") + xlab("Gamma Value") + ggtitle("SVM Error Rate by Gamma Parameter")
```

It looks like model performance on the test set improves at very low levels as gamma increases (until gamma = 0.01) but then deteriorates rapidly as gamma continues to increase. The train error rate continues to improve as gamma increases, indicating clear overfiting post gamma = 0.01.

## 4. Tuning Class

```{r tune_class}
# Initiate list with costs and lists to store error rates
costs <- c(0.1, 0.25, 0.5, 0.75, 1., 2.5, 5., 7.5, 10., 12.5, 15., 17.5, 20.)
cost_train_scores <- rep(0., length(costs))
cost_test_scores <- rep(0., length(costs))

# Loop through each cost value, fitting an svm and calculating training and test error artes
for (cost in 1:length(costs)) {
  model.svm_tune_cost <- svm(Class ~ ., data = dataset_1_train, cost = costs[cost], gamma = 0.01, kernel = "radial")
  cost_train_scores[cost] <- classError(predict(model.svm_tune_cost, dataset_1_train), dataset_1_train$Class)$errorRate
  cost_test_scores[cost] <- classError(predict(model.svm_tune_cost, dataset_1_test), dataset_1_test$Class)$errorRate
}

# Plot training and test error rates
cost_scores <- data.frame(costs = costs, train_scores = cost_train_scores, test_scores = cost_test_scores)
ggplot(cost_scores, aes(costs)) + geom_line(aes(y = train_scores, colour = "Train Error Rate")) + geom_line(aes(y = test_scores, colour = "Test Error Rate")) + ylab("Error Rate") + xlab("Cost Value") + ggtitle("SVM Error Rate by Cost Parameter")
```

It looks like model performance on the test set improves at low levels as cost increases (until cost = 2.5) but then deteriorates slowly as cost continues to increase. The train error rate continues to improve as cost increases, indicating clear overfiting post cost = 2.5.

## 5. Tune Various SVM Models

```{r tuning_parameters}
costs <- 10^c(-10:10)
gammas <- 10^c(-10:10)
```

### Linear Model

```{r tune_svm_linear_model}
# Tune SVM Linear Model
svm.linear.tune <- tune(svm, Class ~ ., data = dataset_1_train, ranges = list(cost = costs, kernel = 'linear'), tunecontrol = tune.control(cross = 5))

# Print tune output
svm.linear.tune

# Plot error rate over tuning parameters
ggplot(svm.linear.tune$performances, mapping = aes(x = cost, y = error)) + geom_line() + scale_x_log10()
svm.linear.tune_error_rate <- classError(predict(svm.linear.tune$best.model, dataset_1_test), dataset_1_test$Class)$errorRate

# Print misclassification rate
sprintf("SVM Linear Misclassification Rate: %.4f", svm.linear.tune_error_rate)
```

### Poly Model

```{r tune_svm_poly_model}
# Tune SVM Poly Model
svm.poly.tune <- tune(svm, Class ~ ., data = dataset_1_train, ranges = list(gamma = gammas, cost = costs, kernel = 'polynomial', degree = 2), tunecontrol = tune.control(cross = 5))

# Print tune output
svm.poly.tune

# Plot error rate over tuning parameters
ggplot(svm.poly.tune$performances, mapping = aes(x = gamma, y = error)) + geom_line() + facet_wrap(~cost, labeller = label_both) + scale_x_log10()

# Print misclassification rate
svm.poly.tune_error_rate <- classError(predict(svm.poly.tune$best.model, dataset_1_test), dataset_1_test$Class)$errorRate
sprintf("SVM Poly Misclassification Rate: %.4f", svm.poly.tune_error_rate)
```

### RBF Model

```{r tune_svm_rbf_model}
# Tune SVM RBF Model
svm.rbf.tune <- tune(svm, Class ~ ., data = dataset_1_train, ranges = list(gamma = gammas, cost = costs, kernel = 'radial'), tunecontrol = tune.control(cross = 5))

# Print tune output
svm.rbf.tune

# Plot error rate over tuning parameters
ggplot(svm.rbf.tune$performances, mapping = aes(x = gamma, y = error)) + geom_line() + facet_wrap(~cost, labeller = label_both) + scale_x_log10()

# Print misclassification rate
svm.rbf.tune_error_rate <- classError(predict(svm.rbf.tune$best.model, dataset_1_test), dataset_1_test$Class)$errorRate
sprintf("SVM RBF Misclassification Rate: %.4f", svm.rbf.tune_error_rate)
```

## 6. Best Model

The linear is the best in terms of model accuracy as it has a lower misclassification rate on the test set when compared to the tuned polynomial and radial-basis models. It also performs much better than a naive classifier that predicts the most common class on all points, which is what the original SVM RBF Model with cost = 1 and gamma = 1 also did.

# Problem 2: Return of the Bayesian Hierarchical Modedl

```{r load_dataset_2}
# Load dataset
dataset_2 <- read.table("datasets/dataset_2.txt", header = TRUE, sep = ",")
```

## 1(a) Pooled Model

```{r pooled_model}
model.pooled <- glm(contraceptive_use ~ living.children, data = dataset_2, family = binomial(link = "logit"))
summary(model.pooled)
```

The number of living children increases the probability that a women uses contraception and this association is statistically significant at the <.001 level.

## 1(b) Unpooled Model

```{r unpooled_model, warning=FALSE}
model.unpooled <- glm(contraceptive_use ~ -1 + living.children*as.factor(district), data = dataset_2, family = binomial(link = "logit"))
summary(model.unpooled)
```

This model formula creates an interaction term between each district and living children, and so each coefficient of the interaction terms represents an individual model for each district since the -1 ensures there is no intercept. It looks like the sign and magnitude of the coefficient for each district differs (though not all statistically significant), indicating that living children have different impacts on contraceptive use depending on the individual district.

## 1(c) Bayesian Hierarchical Logistic Model

```{r bayesian_hiearchical_logistic_model}
model.hierarchical <- MCMChlogit(fixed = contraceptive_use ~ living.children, random = ~ living.children, group = "district", data = dataset_2, r = 2, R = diag(c(1, 0.1)), burnin = 5000, mcmc = 1000)
summary(model.hierarchical)
```

The model returns distributions of intercepts and living children coefficients for each district as well as the estimated probability for each district model. The pooled and unpooled model do not these return probabilities.

## 2(a) Plot living.children Coefficients

```{r plot_living_children_coefficients}
# Get list and number of districts
districts <- unique(dataset_2$district)
n_districts <- length(districts)

# Get pooled coefficients
pooled_coefficients <- rep(model.pooled$coefficients[2], n_districts)

# Get unpooled coeffficents
all_unpooled_coefficients <- model.unpooled$coefficient
n_all_unpooled_coefficients <- length(all_unpooled_coefficients)
unpooled_coefficients <- all_unpooled_coefficients[(n_all_unpooled_coefficients - n_districts + 2):n_all_unpooled_coefficients]
unpooled_coefficients <- c(0, unpooled_coefficients) + all_unpooled_coefficients[1]

# Get bayesian hierarchical model coefficients
all_hierarchical_coefficients <- summary(model.hierarchical$mcmc)$statistics[, 1]
n_all_hierarchical_coefficients <- length(all_hierarchical_coefficients)
hierarchical_coefficients <- all_hierarchical_coefficients[(n_all_hierarchical_coefficients - 5 - n_districts):(n_all_hierarchical_coefficients - 6)]

# Plot coefficients
coefficient_graph_data <- data.frame(districts = districts, observations = as.numeric(table(dataset_2$district)), pooled = pooled_coefficients, unpooled = unpooled_coefficients, hierarchical = hierarchical_coefficients)
ggplot(coefficient_graph_data, aes(districts)) + geom_point(aes(y = pooled, colour = "Pooled Coefficients")) + geom_point(aes(y = unpooled, colour = "Unpooled Coefficients", size = observations)) + geom_point(aes(y = hierarchical, colour = "Bayesian Hierarchical Coefficients", size = observations)) + ylab("Coefficient Value") + xlab("District") + ggtitle("Model Coefficient by District") + ylim(-2, 2)
```

Vertical axis set between -2 and 2, since any coefficients larger than those that have little practical significance and eliminating them increases visibility into the variance in coefficients in the bayesian hierarchical model.

## 2(b) Plot Summary

Both the bayesian hierarchical coefficients and the unpooled coefficients exhibit some variance depending on the district, however the unpooled coefficients appear to have a much higher variance in terms of coefficient value. The bayesian hierarchical coefficients appear to be much more tightly clustered around the pooled coefficient as compared to the unpooled coefficients. In addition, only 2 districts have bayesian hierarchical coefficients that are greater than the pooled coefficient, indicating it may act as a sort of upper-bound for most districts. The bayesian hierarchical coefficients therefore look to be a compromise between the unpooled and pooled coefficients, allowing for variability but still accounting for the overall trend in the dataset.

The number of observations in a particular district appears to effect the variability of the unpooled and bayesian hierarchical coefficients, as those with a lower number of observations tend to have coefficient values further away from the pooled coefficient. This could indicate potential overfitting due to small sample size.

## 3(a) Model Histograms

### Pooled Model

```{r plot_pooled_model}
# Calculate and standardize pooled probabilties
pred.pooled <- predict(model.pooled, dataset_2, type = "response")
dataset_2$pooled_prob <- pred.pooled.standardized

# Plot probabilities
binwidth <- .05
ggplot() + geom_histogram(data = dataset_2[dataset_2$contraceptive_use == 1, ], aes(x = pooled_prob, fill = "1"), binwidth = binwidth) + geom_histogram(data = dataset_2[dataset_2$contraceptive_use == 0, ], aes(x = pooled_prob, y = -..count.., fill = "0"), binwidth = binwidth) + ylab("Frequency") + xlab("Pr(Contraceptive Use)") + ggtitle("Predicted Probabilities for Pooled Model") + scale_fill_manual(values=c("#619CFF", "#F8766D"), name="Contraceptive Use", breaks=c("0", "1"), labels=c("0", "1")) + xlim(-.05, 1.05)
```

### Unpooled Model

```{r plot_unpooled_model}
# Calculate and standardize unpooled probabilties
pred.unpooled <- predict(model.unpooled, dataset_2, type = "response")
dataset_2$unpooled_prob <- pred.unpooled

# Plot probabilities
binwidth <- .05
ggplot() + geom_histogram(data = dataset_2[dataset_2$contraceptive_use == 1, ], aes(x = unpooled_prob, fill = "1"), binwidth = binwidth) + geom_histogram(data = dataset_2[dataset_2$contraceptive_use == 0, ], aes(x = unpooled_prob, y = -..count.., fill = "0"), binwidth = binwidth) + ylab("Frequency") + xlab("Pr(Contraceptive Use)") + ggtitle("Predicted Probabilities for Unpooled Model") + scale_fill_manual(values=c("#619CFF", "#F8766D"), name="Contraceptive Use", breaks=c("0", "1"), labels=c("0", "1")) + xlim(-0.05, 1.05)
```

### Hierarchical Model

```{r plot_hierarchical_model}
# Calculate and standardize pooled probabilties
pred.hierarchical <- model.hierarchical$theta.pred
dataset_2$hierarchical_prob <- pred.hierarchical

# Plot probabilities
binwidth <- .05
ggplot() + geom_histogram(data = dataset_2[dataset_2$contraceptive_use == 1, ], aes(x = hierarchical_prob, fill = "1"), binwidth = binwidth) + geom_histogram(data = dataset_2[dataset_2$contraceptive_use == 0, ], aes(x = hierarchical_prob, y = -..count.., fill = "0"), binwidth = binwidth) + ylab("Frequency") + xlab("Pr(Contraceptive Use)") + ggtitle("Predicted Probabilities for Hierarchical Model") + scale_fill_manual(values=c("#619CFF", "#F8766D"), name="Contraceptive Use", breaks=c("0", "1"), labels=c("0", "1")) + xlim(-0.05, 1.05)
```

### Model Comparison

The pooled model histogram exhibits probabilities for all 4 potential values of living children (with probability increasing as number of children does). It clearly does a poor job of distinguishing between classes as the no contraceptive use dominates regardless of probability. The unpooled model histogram probabilities are more spread out (though still clustered between .25 and .75) and it does a better job differentiating between the classes, particularly at probability values further away from .5. The hierarchical model histogram is more clustered than the unpooled model and appears to differentiate between the classes better around the .5 level than the unpooled model (though neither does particularly well).

## 3(b) Unpooled Hierarchical Comparison

```{r unpooled_hierarchical_comparison}
ggplot(data = dataset_2, aes(x = unpooled_prob)) + geom_point(aes(y = hierarchical_prob)) + geom_line(aes(y = unpooled_prob), colour = "red", size = 1) + ggtitle("Unpooled Model vs. Hierarchical Model")
```

The probabilities of both models look to be positively correlated, however the hierarchical model probabilities appear to be less 'confident' (further away from 0 or 1, closer to .5 than the unpooled model probabilities). This makes sense as the hierarchical model is incorporating the 'prior' of the pooled model, which helps prevent it from overfitting to particular district's data. 