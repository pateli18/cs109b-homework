sds=c(.07,.13,.16)
corrs=matrix(nrow=3,ncol=3,c(1,.3956,.0893,.3596,1,.1731,.0893,.1731,1),byrow=TRUE)
cov(means)
cov(means,means)
corrs
corrs %*% sds
corrs %*% t(sds)
covv = matrix (rep(0,9),nrow=3)
covv
covv[1,1] = corrs[1,1]*means[1]* means[1]
covv
means[1]^2
corrs
corrs[2,3]*sds[2]* sds[3]
corrs[3,2]*sds[2]* sds[3]
covv[3,2] = corrs[3,2]*sds[2]* sds[3]
covv[2,3] = corrs[3,2]*sds[2]* sds[3]
covv
covv[2,2] = sds[2]* sds[2]
covv[3,3] = sds[3]* sds[3]
covv
covv[1,2] = corr[1,2]*sds[1]* sds[2]
covv[1,2] = corrs[1,2]*sds[1]* sds[2]
covv[2,1] = corrs[1,2]*sds[1]* sds[2]
covv
covv[1,3] = corrs[1,3]*sds[1]* sds[3]
covv[3,1] = corrs[1,3]*sds[1]* sds[3]
covv
sds[3]*sds[3]
var.p = t(w) %*% covv %*% w
w = c(.1,.8,.1)
var.p = t(w) %*% covv %*% w
var.p
sqrt(var.p)
source('zivot_code.R')
er = w %.% means
"%.%" <- function(x,y) sum(x*y)
er = w %.% means
er
.05 * .1 + .18 * .8 + .35 * .1
gmin.port = globalMin.portfolio(er,covv)
er
er = w %*% means
er
means*w
er = w * means
gmin.port = globalMin.portfolio(er,covv)
summary(gmin.port)
ef = efficient.frontier(er, covv)
head(ef)
er
ef
tan.port = tangency.portfolio(er,covv,rk.free)
rk.free = 0.025
tan.port = tangency.portfolio(er,covv,rk.free)
tan.port = tangency.portfolio(er,covv,risk.free=rk.free)
er
plot(ef)
7/23
20/36
fname="http://people.fas.harvard.edu/~mparzen/stat107/logisticquest.csv"
mydata=read.csv(fname)
postomorrow=mydata[,2]
gold=mydata[,3]
prevret=mydata[,4]
postomorrow
summary(mydata)
p1 = as.factor(postomorrow)
summary(p1)
mydata$postomorrow = as.factor(mydata$postomorrow)
model = glm(postomorrow~gold+prevret, data=mydata, family=binomial())
summary(model)
fit = predict(model,newdata=newdata,type="response")
newdata = list(gold = 1000, prevret = -0.02)
newdata
fit = predict(model,newdata=newdata,type="response")
summary(fit)
fit
fit
getSymbols("SNA",from="2013-01-01")
getSymbols("SPY",from="2013-01-01")
snaret=monthlyReturn(Ad(SNA))
spyret=monthlyReturn(Ad(SPY))
fit = lm(snaret~spyret)
confint(fit)
summary(fit)
getSymbols("SNA",from="2015-01-01")
getSymbols("IBM",from="2015-01-01")
sna.ret = dailyReturn(Ad(SNA))
ibm.ret = dailyReturn(Ad(IBM))
var.test(sna.ret,ibm.ret)
getSymbols("JNJ",from="2015-01-01", to="2015-12-31")
c = Cl(Ad(JNJ))
Ad(JNJ)
head(Ad(JNJ))
head(JNJ)
Ad(Cl(JNJ))
jnj = adjustOHLC(JNJ)
Cl(jnj)
c = Cl(adjustOHLC(JNJ))
head(c)
?acf.test
??acf.test
c.5 = lag(c,k=-5)
mod = lm(c~c.5)
summary(mod)
acf(residuals(mod))
myvals=function(s1,s2) {
ap1=Ad(getSymbols(s1,auto.assign=FALSE,from=fromDate)) ap2=Ad(getSymbols(s2,auto.assign=FALSE,from=fromDate))
myvals=function(s1,s2) {
ap1=Ad(getSymbols(s1,auto.assign=FALSE,from=fromDate))
ap2=Ad(getSymbols(s2,auto.assign=FALSE,from=fromDate))
n=length(ap2)
nn=n-179
vals=1:nn
i=n
p1 = ap1[(i-179):i]
p2 = ap2[(i-179):i]
fit=lm(p1~-1+p2)
beta=coef(fit)[1]
sprd=p1-beta*p2
sprd=as.numeric(sprd)
cat("Cointegration p=value = ",adf.test(sprd,alternative="stationary",k=0)$p.value,"\n")
cat("Correlation = ",cor(p1,p2),"\n")
}
myvals("IBM","JNJ")
myvals=function(s1,s2) {
ap1=Ad(getSymbols(s1,auto.assign=FALSE,from="2016-01-01"))
ap2=Ad(getSymbols(s2,auto.assign=FALSE,from="2016-01-01"))
n=length(ap2)
nn=n-179
vals=1:nn
i=n
p1 = ap1[(i-179):i]
p2 = ap2[(i-179):i]
fit=lm(p1~-1+p2)
beta=coef(fit)[1]
sprd=p1-beta*p2
sprd=as.numeric(sprd)
cat("Cointegration p=value = ",adf.test(sprd,alternative="stationary",k=0)$p.value,"\n")
cat("Correlation = ",cor(p1,p2),"\n")
}
myvals("IBM","JNJ")
library(quantmod)
library(tseries)
install.packges("tseries")
install.packages("tseries")
library(quantmod)
library(tseries)
myvals=function(s1,s2) {
ap1=Ad(getSymbols(s1,auto.assign=FALSE,from="2016-01-01"))
ap2=Ad(getSymbols(s2,auto.assign=FALSE,from="2016-01-01"))
n=length(ap2)
nn=n-179
vals=1:nn
i=n
p1 = ap1[(i-179):i]
p2 = ap2[(i-179):i]
fit=lm(p1~-1+p2)
beta=coef(fit)[1]
sprd=p1-beta*p2
sprd=as.numeric(sprd)
cat("Cointegration p=value = ",adf.test(sprd,alternative="stationary",k=0)$p.value,"\n")
cat("Correlation = ",cor(p1,p2),"\n")
}
myvals("IBM","JNJ")
x = 1:20
y = x + rnorm(20)
mod = lm(y~x)
summary(mod)
mod = lm(y~x+I(x^2))
summary(mod)
beta = solve(t(x)%*%x)%*%t(x)%*%y
beta
mod = lm(y~x+I(x^3))
summary(mod)
mod = lm(y~x+I(x^2))
summary(mod)
install.packages("PerformanceAnalytics")
library(PerformanceAnalytics)
getSymbols("AAPL",from="2013-01-01",to="2015-12-31")
aaplret = monthlyReturn(Ad(AAPL))
SharpeRatio(aaplret,FUN="StdDev")
table.Stats()
table.Stats(SharpeRatio(aaplret,FUN="StdDev"))
table.Stats(aaplret)
SharpeRatio(aaplret,FUN="StdDev")
.173 + 1.96* 0.0726/6
getSymbols("AAPL",from="2015-01-01",to="2015-12-31")
ret = dailyReturn(Ad(AAPL))
rets = as.numeric(ret)
fit=logspline(rets)
??logspline
install.packages("logspline")
library(logspline)
fit=logspline(rets)
summary(fit)
x=seq(-.02,.02,.01)
fit$fitted
fit=logspline(rets)
fitted.values()
fit$samples
fit=logspline(rets)
summary(fit)
head(fit)
f = function(x) ln(x) / sqrt(x)
f(1)
f = function(x) log(x) / sqrt(x)
f
log(1)
log(2.718)
integrate(f,0,1)
means=c(.05,.18,.35)
sds=c(.07,.13,.16)
corrs=matrix(nrow=3,ncol=3,c(1,.3956,.0893,.3596,1,.1731,.0893,.1731,1),byrow=TRUE)
covv = matrix (rep(0,9),nrow=3)
covv[1,1] = corrs[1,1]*means[1]* means[1]
covv[3,2] = corrs[3,2]*sds[2]* sds[3]
covv[2,3] = corrs[3,2]*sds[2]* sds[3]
covv[2,2] = sds[2]* sds[2]
covv[3,3] = sds[3]* sds[3]
covv[1,2] = corrs[1,2]*sds[1]* sds[2]
covv[2,1] = corrs[1,2]*sds[1]* sds[2]
covv[1,3] = corrs[1,3]*sds[1]* sds[3]
covv[3,1] = corrs[1,3]*sds[1]* sds[3]
w = c(.1,.8,.1)
var.p = t(w) %*% covv %*% w
sqrt(var.p)
source('zivot_code.R')
"%.%" <- function(x,y) sum(x*y)
er = w * means
rk.free = 0.025
gmin.port = globalMin.portfolio(er,covv)
summary(gmin.port)
ef = efficient.frontier(er, covv)
tan.port = tangency.portfolio(er,covv,risk.free=rk.free)
rm(list=ls())
2 * -0.02 + .5 * 0.06
1* (-0.01)+ (-.25)*(0.04)
.5 - 0.02
(0.99)^2/2.04
.4 /4
.4/1024
df = matrix(c(2,1,-2,1,-2,2),nrow=2)
df
df[2,3]=-2
df
c1 = c(3,2,2)
c1
df%*%c
df%*%c1
A = matrix(c(2,1,8,3),nrow=2)
A
solve(A)
Ainv = -solve(A)
Aincv
Ainv
B = c(-6 ,-5)
Ainv%*%B
1.11^2 + 2 * 2.02^2 - 3.01^2
1.11 + 3*2.02 -5 * 3.01 + 8
1 + 3+3-5*3 + 8
A = matrix(c(2,1,-2,-1),nrow=2)
eigen(A)
P  = matrix(c(1,1,2,1),nrow=2)
P
D = matrix(c(0,0,0,e),nrow=2)
D = matrix(c(0,0,0,2.71828),nrow=2)
Pinv = solve(P)
Pinv
A = P%*%D%*%Pinv
A
C = c(3,2)
A = matrix(c(2,1,-2,-1),nrow=2)
A
A%*%C
a = matrix(c(2,2,1,3),nrow=2)
a
solve(a)
ainv = solve(a)
c1 = c(-0.08, -0.04)
ainv%*%c1
1.05^3 + .98^2 -1.05*.98
1.05^2*.98 + 0.98^2
install.packages(c("devtools", "roxygen2", "testthat"))
devtools::dr_devtools()
devtools::dr_devtools()
knitr::opts_chunk$set(echo = TRUE)
for (i in 1:10){
cat ("i:",i,"\n")
}
for (i in 1:5){
cat ("i:",i,"\n")
}
install.packages("MASS")
setwd("~/Projects/Courses/17Spring/109b/2017hw-solutions/HW4/datasets")
install.packages("MLmetrics")
install.packages("caret")
??confusion.matrix
knitr::opts_chunk$set(echo = TRUE)
# load libraries
library("e1071")
library("MLmetrics")
library("caret")
library("lattice")
library("ggplot2")
# read training/testing
train = read.csv('datasets/dataset_1_train.txt')
test = read.csv('datasets/dataset_1_test.txt')
train$Diagnosis <- as.factor(train$Diagnosis)
test$Diagnosis <- as.factor(test$Diagnosis)
# train data
with(train, plot(Biomarker1, Biomarker2, col = Diagnosis, pch = 20))
# test data
with(test, plot(Biomarker1, Biomarker2, col = Diagnosis, pch = 20))
# (a) Linear kernel
lin.svm.model <- svm(Diagnosis ~ ., data = train, cost = 100, kernel = 'linear')
plot(lin.svm.model, train, xlim = c(0, 1), ylim = c(0, 1), grid = 100)
# (b) Polynomial kernel of degree 2 and 3
poly.svm.model <- svm(Diagnosis ~ ., data = train, cost = 100, gamma = 1, kernel = 'polynomial')
plot(poly.svm.model, train, xlim = c(0, 1), ylim = c(0, 1), grid = 100)
# RBF kernel
RBF.svm.model <- svm(Diagnosis ~ ., data = train, cost = 100, gamma = 0.1, kernel = 'radial')
plot(RBF.svm.model, train, xlim = c(0, 1), ylim = c(0, 1), grid = 100, svSymbol = 16, dataSymbol = 16)
tuneResult <- tune(svm, Diagnosis ~ ., data = train, kernel = "radial", cost = 1,
ranges = list(gamma = seq(0,10,0.1))
)
gammas = c(seq(0.1, 5, 0.1), 6:10, seq(11, 100, 10))
model = list()
errors = list()
for(i in 1:length(gammas)){
model[[i]] <- svm(Diagnosis ~ ., data = train, cost = 10, gamma = gammas[i], kernel = 'radial')
errors[[i]] = c(train = mean(train$Diagnosis !=  model[[i]]$fitted), test = mean(predict(model[[i]], test) != test$Diagnosis))
}
errors = do.call('rbind', errors)
plot(gammas, errors[,1], type = 'l', lty = 2, ylab = 'Error', xlab = 'gamma', main = 'Training and Testing Error')
lines(gammas, errors[,2], type = 'l', col = 2, lty = 2)
legend('topright', c('Training Error', 'Testing Error'), col = 1:2, lty = 2)
RBF.svm.model <- svm(Diagnosis ~ ., data = train, cost = 10, gamma = 100, kernel = 'radial')
par(mfrow = c(1, 2))
plot(RBF.svm.model, train, xlim = c(0, 1), ylim = c(0, 1), grid = 100, svSymbol = 16, dataSymbol = 16)
plot(RBF.svm.model, test, xlim = c(0, 1), ylim = c(0, 1), grid = 100, svSymbol = 16, dataSymbol = 16)
set.seed(1)
tuneResult <- tune(svm, Diagnosis ~ .,  data = train, cross = 5, kernel = "radial", ranges = list(gamma = c(seq(0.1, 5, 0.5), 6:10, seq(11, 100, 10)), cost = c(0.5, 1, 10, 30, 50, 100)))
print(tuneResult)
# Draw the tuning graph
plot(tuneResult)
#loading the data
dataset = read.csv('datasets/dataset_2.txt', header = TRUE)
# Make Class a factor, not an integer!
dataset$Class = as.factor(dataset$Class)
#split the data
sample_size <- floor(0.75 * nrow(dataset))
set.seed(123)
train_ind <- sample(seq_len(nrow(dataset)), size = sample_size)
train <- dataset[train_ind, ]
test <- dataset[-train_ind, ]
#split into x/y train/test
x <- subset(train, select=-Class)
y <- train$Class
x_test <- subset(test, select=-Class)
y_test <- test$Class
model <- svm(Class ~ ., data = train, probability = TRUE)
RBF.svm = predict(model, x_test, probability = TRUE)
attr(RBF.svm, 'probabilities')
confusionMatrix.table(train$Class, RBF.svm$fitted)
confusionMatrix(train$Class, RBF.svm$fitted)
RBF.svm$fitted
confusionMatrix(train$Class, RBF.svm)
RBF.svm
summary(RBF.svm)
str(RBF.svm)
str(train$Class)
confusionMatrix(train$Class,RBF.svm)
?RBF.svm
?svm
RBF.svm$x_test
table(train$Class,RBF.svm)
length(RBF.svm)
length(train$Class)
pred = predict(model,x)
length(pred)
table(train$Class,pred)
confusionMatrix(train$Class,pred)
RBF.svm = predict(model, x, probability = TRUE)
attr(RBF.svm, 'probabilities')
confusionMatrix(train$Class, RBF.svm)
confusionMatrix(test$Class, predict(RBF.svm, test))
confusionMatrix(test$Class, predict(RBF.svm, x_test))
y_test = predict(model,x_test)
confusionMatrix(test$Class,y_test)
# training
confusionMatrix(train$Class, RBF.svm)
# testing
confusionMatrix(test$Class, predict(model, x_test))
knitr::opts_chunk$set(echo = TRUE)
# load libraries
library("e1071")
library("MLmetrics")
library("caret")
library("lattice")
library("ggplot2")
# read training/testing
train = read.csv('datasets/dataset_1_train.txt')
test = read.csv('datasets/dataset_1_test.txt')
train$Diagnosis <- as.factor(train$Diagnosis)
test$Diagnosis <- as.factor(test$Diagnosis)
# train data
with(train, plot(Biomarker1, Biomarker2, col = Diagnosis, pch = 20))
# test data
with(test, plot(Biomarker1, Biomarker2, col = Diagnosis, pch = 20))
# (a) Linear kernel
lin.svm.model <- svm(Diagnosis ~ ., data = train, cost = 100, kernel = 'linear')
plot(lin.svm.model, train, xlim = c(0, 1), ylim = c(0, 1), grid = 100)
# (b) Polynomial kernel of degree 2 and 3
poly.svm.model <- svm(Diagnosis ~ ., data = train, cost = 100, gamma = 1, kernel = 'polynomial')
plot(poly.svm.model, train, xlim = c(0, 1), ylim = c(0, 1), grid = 100)
# RBF kernel
RBF.svm.model <- svm(Diagnosis ~ ., data = train, cost = 100, gamma = 0.1, kernel = 'radial')
plot(RBF.svm.model, train, xlim = c(0, 1), ylim = c(0, 1), grid = 100, svSymbol = 16, dataSymbol = 16)
tuneResult <- tune(svm, Diagnosis ~ ., data = train, kernel = "radial", cost = 1,
ranges = list(gamma = seq(0,10,0.1))
)
gammas = c(seq(0.1, 5, 0.1), 6:10, seq(11, 100, 10))
model = list()
errors = list()
for(i in 1:length(gammas)){
model[[i]] <- svm(Diagnosis ~ ., data = train, cost = 10, gamma = gammas[i], kernel = 'radial')
errors[[i]] = c(train = mean(train$Diagnosis !=  model[[i]]$fitted), test = mean(predict(model[[i]], test) != test$Diagnosis))
}
errors = do.call('rbind', errors)
plot(gammas, errors[,1], type = 'l', lty = 2, ylab = 'Error', xlab = 'gamma', main = 'Training and Testing Error')
lines(gammas, errors[,2], type = 'l', col = 2, lty = 2)
legend('topright', c('Training Error', 'Testing Error'), col = 1:2, lty = 2)
RBF.svm.model <- svm(Diagnosis ~ ., data = train, cost = 10, gamma = 100, kernel = 'radial')
par(mfrow = c(1, 2))
plot(RBF.svm.model, train, xlim = c(0, 1), ylim = c(0, 1), grid = 100, svSymbol = 16, dataSymbol = 16)
plot(RBF.svm.model, test, xlim = c(0, 1), ylim = c(0, 1), grid = 100, svSymbol = 16, dataSymbol = 16)
set.seed(1)
tuneResult <- tune(svm, Diagnosis ~ .,  data = train, cross = 5, kernel = "radial", ranges = list(gamma = c(seq(0.1, 5, 0.5), 6:10, seq(11, 100, 10)), cost = c(0.5, 1, 10, 30, 50, 100)))
print(tuneResult)
# Draw the tuning graph
plot(tuneResult)
#loading the data
dataset = read.csv('datasets/dataset_2.txt', header = TRUE)
# Make Class a factor, not an integer!
dataset$Class = as.factor(dataset$Class)
#split the data
sample_size <- floor(0.75 * nrow(dataset))
set.seed(123)
train_ind <- sample(seq_len(nrow(dataset)), size = sample_size)
train <- dataset[train_ind, ]
test <- dataset[-train_ind, ]
#split into x/y train/test
x <- subset(train, select=-Class)
y <- train$Class
x_test <- subset(test, select=-Class)
y_test <- test$Class
model <- svm(Class ~ ., data = train, probability = TRUE)
RBF.svm = predict(model, x, probability = TRUE)
attr(RBF.svm, 'probabilities')
# training
confusionMatrix(train$Class, RBF.svm)
# testing
confusionMatrix(test$Class, predict(model, x_test))
# tuneResult.linear <- tune(svm, Class ~ ., data = train, kernel = "linear",
# ranges = list(cost = 10^(-5:0)))
# tuneResult.poly <- tune(svm, Class ~ ., data = train, kernel = "polynomial", , degree = 2, ranges = list(# tuneResult.RBF <- tune(svm, Class ~ ., data = train, kernel = "radial",
# ranges = list(gamma = 10^(-6:0), cost = 2^(-1:5)))
# tuneResult.linear
# tuneResult.poly
# tuneResult.RBF
bestlinear.svm.model <- svm(Class ~ ., data = train, kernel = 'linear', cost = 0.0625)
bestpoly.svm.model <- svm(Class ~ ., data = train, kernel = 'polynomial', cost = 32, gamma = 0.1)
bestRBF.svm.model <- svm(Class ~ ., data = train, kernel = 'radial', cost = 8, gamma = 0.1)
1-mean(train$Class == bestlinear.svm.model$fitted)
#best linear model
1-mean(train$Class == bestpoly.svm.model$fitted)
#best polynomial model
1-mean(train$Class == bestRBF.svm.model$fitted)
#best RBF model
1-mean(train$Class == bestRBF.svm.model$fitted)
# confusion.matrix(test$Class, predict(bestlinear.svm.model, test))
# confusion.matrix(test$Class, predict(bestpoly.svm.model, test))
# confusion.matrix(test$Class, predict(bestRBF.svm.model, test))
bestlinear.error = mean(test$Class != predict(bestlinear.svm.model, test))
bestpoly.error = mean(test$Class != predict(bestpoly.svm.model, test))
bestRBF.error = mean(test$Class != predict(bestRBF.svm.model, test))
c('lin' = bestlinear.error, 'poly' = bestpoly.error, 'rbf' = bestRBF.error)
#class frequency
as.data.frame(table(dataset$Class))
#SVM model that optimizes the balanced error rate during the training step
svm_tune2 <- tune(svm, train.x=x, train.y=y,
kernel="radial", ranges=list(cost=10^(-1:2), gamma=c(.5,1,2)),
class.weights= c("1" = 127, "2" = 101, "3" = 255, "4" = 361,
"5" = 580, "6" = 3963, "7" = 59, "8" = 613))
?tune
#class frequency
as.data.frame(table(dataset$Class))
#SVM model that optimizes the balanced error rate during the training step
svm_tune2 <- tune(svm, train.x=x, train.y=y,
kernel="radial", ranges=list(cost=10^(-1:2), gamma=c(.5,1,2)))
#,
#                 class.weights= c("1" = 127, "2" = 101, "3" = 255, "4" = 361,
#                                  "5" = 580, "6" = 3963, "7" = 59, "8" = 613))
#class frequency
as.data.frame(table(dataset$Class))
#SVM model that optimizes the balanced error rate during the training step
svm_tune2 <- tune(svm, train.x=x, train.y=y,
kernel="radial", ranges=list(cost=10^(-1:2), gamma=c(.5,1,2)),
class.weights= c("1" = 127, "2" = 101, "3" = 255, "4" = 361,
"5" = 580, "6" = 3963, "7" = 59, "8" = 613))
